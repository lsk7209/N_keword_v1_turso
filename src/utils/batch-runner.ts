import { getTursoClient, getCurrentTimestamp } from '@/utils/turso';
import { processSeedKeyword, bulkDeferredInsert, Keyword } from '@/utils/mining-engine';
import { fetchDocumentCount } from '@/utils/naver-api';
import { keyManager } from '@/utils/key-manager';

type MiningMode = 'NORMAL' | 'TURBO';
type MiningTask = 'all' | 'expand' | 'fill_docs';

export interface MiningBatchOptions {
    task?: MiningTask;
    mode?: MiningMode;
    seedCount?: number;
    expandBatch?: number;
    expandConcurrency?: number;
    fillDocsBatch?: number;
    fillDocsConcurrency?: number;
    maxRunMs?: number;
    minSearchVolume?: number;
}

function clampInt(val: unknown, min: number, max: number, fallback: number) {
    const n = typeof val === 'number' ? val : Number(val);
    if (!Number.isFinite(n)) return fallback;
    return Math.max(min, Math.min(max, Math.trunc(n)));
}

async function mapWithConcurrency<T, R>(
    items: T[],
    concurrency: number,
    worker: (item: T, idx: number) => Promise<R>
): Promise<R[]> {
    const results: R[] = new Array(items.length);
    let nextIndex = 0;
    const workers = new Array(Math.max(1, concurrency)).fill(null).map(async () => {
        while (true) {
            const idx = nextIndex++;
            if (idx >= items.length) return;
            // ğŸš€ Stability Optimization: Small jitter between requests to avoid burst 429s
            if (nextIndex > concurrency) {
                await new Promise(r => setTimeout(r, Math.random() * 50 + 10));
            }
            results[idx] = await worker(items[idx], idx);
        }
    });
    await Promise.all(workers);
    return results;
}

export interface ExpandResult {
    processedSeeds: number;
    totalSaved: number;
    details: string[];
    error?: string;
}

export interface FillDocsResult {
    processed: number;
    failed: number;
    skipped: number;
    details: string[];
    error?: string;
}

export interface MiningBatchResult {
    expand?: ExpandResult;
    fillDocs?: FillDocsResult;
}

export async function runMiningBatch(options: MiningBatchOptions = {}): Promise<MiningBatchResult> {
    const db = getTursoClient();

    // íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œê¹…
    const start = Date.now();
    console.log('[BatchRunner] ğŸš€ Starting Parallel Mining Batch...');

    // ê¸°ë³¸ ì„¤ì •
    let mode: MiningMode = 'TURBO';
    const task: MiningTask = (options.task === 'expand' || options.task === 'fill_docs' || options.task === 'all')
        ? options.task
        : 'all';

    const maxRunMs = clampInt(options.maxRunMs, 10_000, 58_000, 58_000);
    const deadline = start + maxRunMs;

    // API í‚¤ ìˆ˜ì— ë”°ë¥¸ ë™ì  í™•ì¥
    const searchKeyCount = keyManager.getKeyCount('SEARCH');
    const adKeyCount = keyManager.getKeyCount('AD');

    // ğŸš€ íšê¸°ì  ìµœì í™”: API í‚¤ ìˆ˜ë¥¼ ìµœëŒ€í•œ í™œìš©
    const baseExpandConcurrency = Math.min(100, Math.max(20, adKeyCount * 5));
    const baseFillConcurrency = Math.min(500, Math.max(100, searchKeyCount * 15));

    const EXPAND_CONCURRENCY = clampInt(options.expandConcurrency, 1, 200, baseExpandConcurrency);
    const FILL_DOCS_CONCURRENCY = clampInt(options.fillDocsConcurrency, 1, 500, baseFillConcurrency);

    const expandBatchBase = Math.max(100, EXPAND_CONCURRENCY * 10);
    const fillDocsBatchBase = Math.max(200, FILL_DOCS_CONCURRENCY * 5);

    const EXPAND_BATCH = clampInt(options.expandBatch, 1, 2000, expandBatchBase);
    const FILL_DOCS_BATCH = clampInt(options.fillDocsBatch, 1, 2500, fillDocsBatchBase);

    const MIN_SEARCH_VOLUME = Math.max(100, clampInt(options.minSearchVolume, 0, 50_000, 100));

    console.log(`[BatchRunner] Mode: ${mode}, Keys(S/A): ${searchKeyCount}/${adKeyCount}, Task: ${task}`);
    console.log(`[BatchRunner] Config: Expand(Batch:${EXPAND_BATCH}, Conc:${EXPAND_CONCURRENCY}), FillDocs(Batch:${FILL_DOCS_BATCH}, Conc:${FILL_DOCS_CONCURRENCY}), MaxRunMs: ${maxRunMs}`);

    // ê²°ê³¼ ê°ì²´
    let result: MiningBatchResult = {};

    const tasks: Promise<void>[] = [];

    if (task === 'expand' || task === 'all') {
        tasks.push(
            runExpandTask(EXPAND_BATCH, EXPAND_CONCURRENCY, MIN_SEARCH_VOLUME, deadline)
                .then(res => {
                    if (res) {
                        result.expand = res;
                        console.log(`[BatchRunner] âœ… Expand task finished: ${res.processedSeeds} processed, ${res.totalSaved} saved`);
                    } else {
                        console.warn('[BatchRunner] âš ï¸ Expand task returned null');
                    }
                })
                .catch(err => {
                    console.error('[BatchRunner] âŒ Expand task fatal error:', err);
                    result.expand = { processedSeeds: 0, totalSaved: 0, details: [], error: err.message };
                })
        );
    }

    if (task === 'fill_docs' || task === 'all') {
        tasks.push(
            runFillDocsTask(FILL_DOCS_BATCH, FILL_DOCS_CONCURRENCY, deadline)
                .then(res => {
                    if (res) {
                        result.fillDocs = res;
                        console.log(`[BatchRunner] âœ… FillDocs task finished: ${res.processed} items updated`);
                    } else {
                        console.warn('[BatchRunner] âš ï¸ FillDocs task returned null');
                    }
                })
                .catch(err => {
                    console.error('[BatchRunner] âŒ FillDocs task fatal error:', err);
                    result.fillDocs = { processed: 0, failed: 0, skipped: 0, details: [], error: err.message };
                })
        );
    }

    await Promise.all(tasks);

    const end = Date.now();
    console.log(`[BatchRunner] ğŸ Parallel Batch Completed in ${(end - start)}ms`);

    return result;
}

interface SeedItem {
    id: string;
    keyword: string;
    total_search_cnt: number;
}

// Expand ì‘ì—… í•¨ìˆ˜
async function runExpandTask(batchSize: number, concurrency: number, minSearchVolume: number, deadline: number): Promise<ExpandResult | null> {
    const db = getTursoClient();

    // ğŸš€ Read/Write Optimization: Use batch for atomic claim
    let seedsData: SeedItem[] = [];
    try {
        const selectResult = await db.execute({
            sql: `SELECT id, keyword, total_search_cnt FROM keywords
                  WHERE (is_expanded = 0)
                     OR (is_expanded = 2 AND updated_at < datetime('now', '-2 hours')) -- Retry stuck ones
                     OR (is_expanded = 1 AND updated_at < datetime('now', '-7 days'))
                  ORDER BY
                      CASE
                          WHEN is_expanded = 0 THEN 0
                          WHEN is_expanded = 2 THEN 1
                          WHEN is_expanded = 1 THEN 2
                      END,
                      total_search_cnt DESC
                  LIMIT ?`,
            args: [batchSize]
        });

        seedsData = selectResult.rows.map(row => ({
            id: row.id as string,
            keyword: row.keyword as string,
            total_search_cnt: row.total_search_cnt as number
        }));

        if (seedsData.length > 0) {
            const ids = seedsData.map(s => s.id);
            const placeholders = ids.map(() => '?').join(',');

            // Mark as processing in one batch call alongside the previous read concept?
            // Since Turso is remote, we minimize round trips.
            await db.execute({
                sql: `UPDATE keywords SET is_expanded = 2, updated_at = ? WHERE id IN (${placeholders})`,
                args: [getCurrentTimestamp(), ...ids]
            });
            console.log(`[Expand] Claimed ${seedsData.length} seeds`);
        }
    } catch (err: any) {
        console.error('[Expand] Failed to claim seeds:', err.message);
        return null;
    }

    if (seedsData.length === 0) {
        console.log('[Expand] No seeds available for expansion');
        return {
            processedSeeds: 0,
            totalSaved: 0,
            details: []
        };
    }
    console.log(`[Batch] EXPAND: Claimed ${seedsData.length} seeds (Concurrency ${concurrency})`);

    // ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²°ê³¼ ì¶•ì 
    let memoryKeywordBuffer: Keyword[] = [];
    let memorySeedUpdates: { id: string, status: 'success' | 'failed' }[] = [];

    const expandResults = await mapWithConcurrency(seedsData, concurrency, async (seed) => {
        if (Date.now() > (deadline - 2500)) {
            return { status: 'skipped_deadline', seed };
        }

        try {
            const result = await processSeedKeyword(seed.keyword, 0, true, minSearchVolume);
            if (result.saved > 0) {
                memoryKeywordBuffer.push(...result.items);
                memorySeedUpdates.push({ id: seed.id, status: 'success' });
                return { status: 'fulfilled', seed, saved: result.saved };
            } else {
                memorySeedUpdates.push({ id: seed.id, status: 'failed' });
                return { status: 'fulfilled', seed, saved: 0 };
            }
        } catch (e: any) {
            console.error(`[BatchRunner] Seed Failed: ${seed.keyword} - ${e.message}`);
            memorySeedUpdates.push({ id: seed.id, status: 'failed' });
            return { status: 'rejected', seed, error: e.message };
        }
    });

    // ë°°ì¹˜ ì‚½ì…
    if (memoryKeywordBuffer.length > 0) {
        try {
            await bulkDeferredInsert(memoryKeywordBuffer);
            console.log(`[BatchRunner] âš¡ Deferred Bulk Insert: ${memoryKeywordBuffer.length} keywords`);
        } catch (e) {
            console.error('[BatchRunner] Bulk insert failed:', e);
        }
    }

    // ìƒíƒœ ì—…ë°ì´íŠ¸
    const successIds = memorySeedUpdates.filter(s => s.status === 'success').map(s => s.id);
    const failIds = memorySeedUpdates.filter(s => s.status === 'failed').map(s => s.id);
    const skippedSeeds = expandResults.filter(r => r.status === 'skipped_deadline').map(r => r.seed);

    if (successIds.length > 0) {
        const placeholders = successIds.map(() => '?').join(',');
        await db.execute({
            sql: `UPDATE keywords SET is_expanded = 1 WHERE id IN (${placeholders})`,
            args: successIds
        });
    }

    if (failIds.length > 0) {
        const placeholders = failIds.map(() => '?').join(',');
        await db.execute({
            sql: `UPDATE keywords SET is_expanded = 0 WHERE id IN (${placeholders})`,
            args: failIds
        });
    }

    // ìŠ¤í‚µëœ ì‹œë“œ ë¡¤ë°±
    if (skippedSeeds.length > 0) {
        const skippedIds = skippedSeeds.map(s => s.id);
        const placeholders = skippedIds.map(() => '?').join(',');
        await db.execute({
            sql: `UPDATE keywords SET is_expanded = 0 WHERE id IN (${placeholders})`,
            args: skippedIds
        });
    }

    const succeeded = expandResults.filter(r => r.status === 'fulfilled');
    return {
        processedSeeds: seedsData.length,
        totalSaved: succeeded.reduce((sum, r) => (sum + (r.saved || 0)), 0),
        details: expandResults.map((r) =>
            r.status === 'fulfilled' ? `${r.seed.keyword} (+${r.saved})` :
                r.status === 'rejected' ? `${r.seed.keyword} (rejected: ${r.error})` :
                    `${r.seed.keyword} (${r.status})`
        )
    };
}

// Fill Docs ì‘ì—… í•¨ìˆ˜
async function runFillDocsTask(batchSize: number, concurrency: number, deadline: number): Promise<FillDocsResult | null> {
    const db = getTursoClient();

    // ğŸš€ Read/Write Optimization: Atomic claim
    let docsToFill: SeedItem[] = [];
    try {
        const selectResult = await db.execute({
            sql: `SELECT id, keyword, total_search_cnt FROM keywords
                  WHERE (total_doc_cnt IS NULL)
                     OR (total_doc_cnt = -2 AND updated_at < datetime('now', '-2 hours')) -- Retry stuck ones
                  ORDER BY total_search_cnt DESC
                  LIMIT ?`,
            args: [batchSize]
        });

        docsToFill = selectResult.rows.map(row => ({
            id: row.id as string,
            keyword: row.keyword as string,
            total_search_cnt: row.total_search_cnt as number
        }));

        if (docsToFill.length > 0) {
            const ids = docsToFill.map(d => d.id);
            const placeholders = ids.map(() => '?').join(',');
            await db.execute({
                sql: `UPDATE keywords SET total_doc_cnt = -2, updated_at = ? WHERE id IN (${placeholders})`,
                args: [getCurrentTimestamp(), ...ids]
            });
            console.log(`[FillDocs] Claimed ${docsToFill.length} keywords`);
        }
    } catch (err: any) {
        console.error('[FillDocs] Failed to claim keywords:', err.message);
        return null;
    }

    if (docsToFill.length === 0) {
        console.log('[BatchRunner] No keywords need document count');
        return {
            processed: 0,
            failed: 0,
            skipped: 0,
            details: []
        };
    }

    console.log(`[BatchRunner] FILL_DOCS: Claimed ${docsToFill.length} items (Concurrency ${concurrency})`);

    // ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²°ê³¼ ì¶•ì 
    let memoryDocUpdates: { id: string, counts: any }[] = [];

    const processedResults = await mapWithConcurrency(docsToFill, concurrency, async (item) => {
        if (Date.now() > (deadline - 1000)) {
            return { status: 'skipped_deadline', item };
        }
        try {
            const counts = await fetchDocumentCount(item.keyword);
            memoryDocUpdates.push({ id: item.id, counts });
            return { status: 'fulfilled', item, counts };
        } catch (e: any) {
            console.error(`[BatchRunner] Error filling ${item.keyword}: ${e.message}`);
            return { status: 'rejected', keyword: item.keyword, item, error: e.message };
        }
    });

    // ë°°ì¹˜ ì—…ë°ì´íŠ¸ (ì•ˆì „í•˜ê²Œ 200ê°œì”© ì²­í¬ - Rule ì¤€ìˆ˜)
    if (memoryDocUpdates.length > 0) {
        const CHUNK_SIZE = 200;
        for (let i = 0; i < memoryDocUpdates.length; i += CHUNK_SIZE) {
            const chunk = memoryDocUpdates.slice(i, i + CHUNK_SIZE);
            const updateStatements = chunk.map(({ id, counts }) => ({
                sql: `UPDATE keywords SET
                    total_doc_cnt = ?, blog_doc_cnt = ?, cafe_doc_cnt = ?,
                    web_doc_cnt = ?, news_doc_cnt = ?, updated_at = ?
                    WHERE id = ?`,
                args: [
                    counts.total, counts.blog || 0, counts.cafe || 0,
                    counts.web || 0, counts.news || 0, getCurrentTimestamp(), id
                ]
            }));

            try {
                await db.batch(updateStatements);
                console.log(`[BatchRunner] âš¡ Bulk Doc Update: chunk ${Math.floor(i / CHUNK_SIZE) + 1} (${chunk.length} items)`);
            } catch (e: any) {
                console.error(`[BatchRunner] Bulk doc update failed for chunk starting at ${i}:`, e.message);
            }
        }
    }

    const succeeded = processedResults.filter(r => r.status === 'fulfilled');
    const failed = processedResults.filter(r => r.status === 'rejected');
    const skipped = processedResults.filter(r => r.status === 'skipped_deadline');

    // ğŸ”´ ì‹¤íŒ¨(rejected) ë˜ëŠ” ìŠ¤í‚µ(skipped) í•­ëª© ë¡¤ë°± (ë§¤ìš° ì¤‘ìš”: -2 ìƒíƒœ ê³ ì°© ë°©ì§€)
    const rollbackIds: string[] = [
        ...failed.map(r => r.item?.id).filter((id): id is string => !!id),
        ...skipped.map(r => r.item?.id).filter((id): id is string => !!id)
    ];

    if (rollbackIds.length > 0) {
        console.log(`[BatchRunner] ğŸ”„ Rolling back ${rollbackIds.length} failed/skipped items to NULL`);
        // ë¡¤ë°±ë„ 100ê°œì”© ì²­í¬
        for (let i = 0; i < rollbackIds.length; i += 100) {
            const chunk = rollbackIds.slice(i, i + 100);
            const placeholders = chunk.map(() => '?').join(',');
            try {
                await db.execute({
                    sql: `UPDATE keywords SET total_doc_cnt = NULL WHERE id IN (${placeholders})`,
                    args: chunk
                });
            } catch (err: any) {
                console.error('[BatchRunner] Rollback failed:', err.message);
            }
        }
    }

    return {
        processed: succeeded.length,
        failed: failed.length,
        skipped: skipped.length,
        details: processedResults.map((r: any) => {
            if (r.status === 'fulfilled') return `${r.item.keyword}: ${r.counts.total}`;
            if (r.status === 'rejected') return `${r.keyword || r.item?.keyword || 'unknown'}: ERROR`;
            return `${r.item?.keyword || 'unknown'}: SKIPPED`;
        })
    };
}


import { getTursoClient, getCurrentTimestamp } from '@/utils/turso';
import { processSeedKeyword } from '@/utils/mining-engine';
import { fetchDocumentCount } from '@/utils/naver-api';
import { keyManager } from '@/utils/key-manager';

type MiningMode = 'NORMAL' | 'TURBO';
type MiningTask = 'all' | 'expand' | 'fill_docs';

export interface MiningBatchOptions {
    task?: MiningTask;
    mode?: MiningMode; // optional override
    seedCount?: number;
    expandBatch?: number;
    expandConcurrency?: number;
    fillDocsBatch?: number;
    fillDocsConcurrency?: number; // keywords concurrently fetching doc counts
    maxRunMs?: number; // hard deadline to avoid Vercel timeout (default: 55s)
    minSearchVolume?: number;
}

function clampInt(val: unknown, min: number, max: number, fallback: number) {
    const n = typeof val === 'number' ? val : Number(val);
    if (!Number.isFinite(n)) return fallback;
    return Math.max(min, Math.min(max, Math.trunc(n)));
}

async function mapWithConcurrency<T, R>(
    items: T[],
    concurrency: number,
    worker: (item: T, idx: number) => Promise<R>
): Promise<R[]> {
    const results: R[] = new Array(items.length);
    let nextIndex = 0;
    const workers = new Array(Math.max(1, concurrency)).fill(null).map(async () => {
        while (true) {
            const idx = nextIndex++;
            if (idx >= items.length) return;
            results[idx] = await worker(items[idx], idx);
        }
    });
    await Promise.all(workers);
    return results;
}

export async function runMiningBatch(options: MiningBatchOptions = {}) {
    const db = getTursoClient();

    // ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Î°úÍπÖ
    const start = Date.now();
    console.log('[Batch] Starting Parallel Mining Batch...');

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: DB ÏùΩÍ∏∞ ÏµúÏÜåÌôî - options.mode Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í TURBO ÏÇ¨Ïö©
    // settings ÌÖåÏù¥Î∏î Ï°∞ÌöåÎäî ÏÑ†ÌÉùÏ†ÅÏúºÎ°úÎßå ÏàòÌñâ (DB ÏùΩÍ∏∞ 1Ìöå Ï†àÏïΩ)
    let mode: MiningMode = 'TURBO'; // Í∏∞Î≥∏Í∞íÏùÄ TURBO (ÎåÄÎüâ ÏàòÏßë ÏµúÏ†ÅÌôî)
    let isTurboMode = true;
    
    if (options.mode === 'NORMAL' || options.mode === 'TURBO') {
        mode = options.mode;
        isTurboMode = mode === 'TURBO';
    } else {
        // options.modeÍ∞Ä ÏóÜÏùÑ ÎïåÎßå DB Ï°∞Ìöå (ÏµúÏÜåÌôî)
        try {
            const settingResult = await db.execute({
                sql: 'SELECT value FROM settings WHERE key = ?',
                args: ['mining_mode']
            });
            if (settingResult.rows.length > 0) {
                const rawValue = (settingResult.rows[0] as any).value;
                if (typeof rawValue === 'string') {
                    mode = rawValue.replace(/^"|"$/g, '').toUpperCase() as MiningMode;
                } else {
                    mode = String(rawValue).toUpperCase() as MiningMode;
                }
                if (mode !== 'NORMAL' && mode !== 'TURBO') {
                    mode = 'TURBO';
                }
                isTurboMode = mode === 'TURBO';
            }
        } catch (e) {
            // DB Ï°∞Ìöå Ïã§Ìå® Ïãú Í∏∞Î≥∏Í∞í TURBO ÏÇ¨Ïö©
            console.warn('[Batch] Failed to read mining_mode from DB, using TURBO default');
        }
    }

    const task: MiningTask = (options.task === 'expand' || options.task === 'fill_docs' || options.task === 'all')
        ? options.task
        : 'all';

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: ÏµúÎåÄ Ïã§Ìñâ ÏãúÍ∞Ñ ÌôïÎåÄ (55Ï¥à ‚Üí 58Ï¥à)Î°ú Îçî ÎßéÏùÄ Ï≤òÎ¶¨
    const maxRunMs = clampInt(options.maxRunMs, 10_000, 58_000, 58_000);
    const deadline = start + maxRunMs;

    // ÌÑ∞Î≥¥Î™®Îìú: API ÌÇ§ ÏàòÏóê Îî∞Î•∏ ÎèôÏ†Å ÌôïÏû• (Aggressive)
    const searchKeyCount = keyManager.getKeyCount('SEARCH');
    const adKeyCount = keyManager.getKeyCount('AD');

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: ÏµúÎåÄ ÏÑ±Îä•ÏùÑ ÏúÑÌïú Í≥µÍ≤©Ï†Å ÏÑ§Ï†ï (API ÌÇ§ ÏµúÎåÄ ÌôúÏö©)
    // AD Key: Í∞úÎãπ 8-10Î∞∞ (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî ÏµúÎåÄÌïú ÌôúÏö©)
    // ÏµúÏÜå 20Í∞ú, ÌÇ§Í∞Ä ÎßéÏùÑÏàòÎ°ù Ï¶ùÍ∞Ä (ÏµúÎåÄ Ï†úÌïú ÏóÜÏùå)
    let baseExpandConcurrency = isTurboMode 
        ? Math.max(20, adKeyCount * 10)  // ÌÑ∞Î≥¥: ÌÇ§Îãπ 10Î∞∞, ÏµúÏÜå 20 (5Î∞∞ ‚Üí 10Î∞∞Î°ú Ï¶ùÍ∞Ä)
        : Math.max(4, adKeyCount * 2);  // ÏùºÎ∞ò: ÌÇ§Îãπ 2Î∞∞, ÏµúÏÜå 4
    
    // Search Key: Í∞úÎãπ 10-12Î∞∞ (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî ÏµúÎåÄÌïú ÌôúÏö©)
    // ÏµúÏÜå 50Í∞ú, ÌÇ§Í∞Ä ÎßéÏùÑÏàòÎ°ù Ï¶ùÍ∞Ä (ÏµúÎåÄ Ï†úÌïú ÏóÜÏùå)
    let baseFillConcurrency = isTurboMode
        ? Math.max(50, searchKeyCount * 12)  // ÌÑ∞Î≥¥: ÌÇ§Îãπ 12Î∞∞, ÏµúÏÜå 50 (6Î∞∞ ‚Üí 12Î∞∞Î°ú Ï¶ùÍ∞Ä)
        : Math.max(20, searchKeyCount * 3); // ÏùºÎ∞ò: ÌÇ§Îãπ 3Î∞∞, ÏµúÏÜå 20

    console.log(`[Batch] üöÄ TURBO Mode: Key-based concurrency: AD keys=${adKeyCount} ‚Üí expand=${baseExpandConcurrency}, SEARCH keys=${searchKeyCount} ‚Üí fill=${baseFillConcurrency}`);

    const SEED_COUNT = clampInt(options.seedCount, 0, 50, isTurboMode ? 20 : 5);

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: ÎèôÏãúÏÑ± Ï†úÌïúÏùÑ ÌÅ¨Í≤å ÌôïÎåÄ (API ÌÇ§ ÏµúÎåÄ ÌôúÏö©)
    // EXPAND: ÏµúÎåÄ 500ÍπåÏßÄ ÌóàÏö© (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî Îçî ÎßéÏùÄ ÎèôÏãú Ï≤òÎ¶¨)
    const EXPAND_CONCURRENCY = clampInt(options.expandConcurrency, 1, isTurboMode ? 500 : 100, baseExpandConcurrency);
    // FILL_DOCS: ÏµúÎåÄ 1000ÍπåÏßÄ ÌóàÏö© (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî Îçî ÎßéÏùÄ ÎèôÏãú Ï≤òÎ¶¨)
    const FILL_DOCS_CONCURRENCY = clampInt(options.fillDocsConcurrency, 1, isTurboMode ? 1000 : 400, baseFillConcurrency);

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: Î∞∞Ïπò ÌÅ¨Í∏∞Î•º ÏµúÎåÄÌïú ÌÅ¨Í≤å ÏÑ§Ï†ï (API ÌÇ§ ÏµúÎåÄ ÌôúÏö©)
    // EXPAND: ÎèôÏãúÏÑ±Ïùò 15-20Î∞∞ (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî Îçî ÎßéÏùÄ ÏãúÎìú Ï≤òÎ¶¨)
    const expandBatchBase = isTurboMode
        ? Math.max(200, baseExpandConcurrency * 20)  // ÌÑ∞Î≥¥: 20Î∞∞, ÏµúÏÜå 200 (12Î∞∞ ‚Üí 20Î∞∞Î°ú Ï¶ùÍ∞Ä)
        : Math.max(50, baseExpandConcurrency * 8);   // ÏùºÎ∞ò: 8Î∞∞, ÏµúÏÜå 50
    
    // FILL_DOCS: ÎèôÏãúÏÑ±Ïùò 15-20Î∞∞ (ÌÑ∞Î≥¥Î™®ÎìúÏóêÏÑúÎäî Îçî ÎßéÏùÄ ÌÇ§ÏõåÎìú Ï≤òÎ¶¨)
    const fillDocsBatchBase = isTurboMode
        ? Math.max(500, baseFillConcurrency * 20)  // ÌÑ∞Î≥¥: 20Î∞∞, ÏµúÏÜå 500 (10Î∞∞ ‚Üí 20Î∞∞Î°ú Ï¶ùÍ∞Ä)
        : Math.max(100, baseFillConcurrency * 5);  // ÏùºÎ∞ò: 5Î∞∞, ÏµúÏÜå 100

    // üöÄ ÌÑ∞Î≥¥Î™®Îìú: Î∞∞Ïπò ÌÅ¨Í∏∞ Ï†úÌïúÏùÑ ÌÅ¨Í≤å ÌôïÎåÄ (API ÌÇ§ ÏµúÎåÄ ÌôúÏö©)
    const EXPAND_BATCH = clampInt(options.expandBatch, 1, isTurboMode ? 5000 : 1000, expandBatchBase);
    const FILL_DOCS_BATCH = clampInt(options.fillDocsBatch, 1, isTurboMode ? 20000 : 5000, fillDocsBatchBase);

    // ÏµúÏÜå Í≤ÄÏÉâÎüâ 1000 Í∞ïÏ†ú (ÏøºÎ¶¨ ÌååÎùºÎØ∏ÌÑ∞Î°ú 0Ïù¥ Ï†ÑÎã¨ÎêòÏñ¥ÎèÑ ÏµúÏÜå 1000 Ï†ÅÏö©)
    const MIN_SEARCH_VOLUME = Math.max(1000, clampInt(options.minSearchVolume, 0, 50_000, 1000));

    console.log(`[Batch] Mode: ${isTurboMode ? 'TURBO' : 'NORMAL'}, Keys(S/A): ${searchKeyCount}/${adKeyCount}, Task: ${task}`);
    console.log(`[Batch] Config: Expand(Batch:${EXPAND_BATCH}, Conc:${EXPAND_CONCURRENCY}), FillDocs(Batch:${FILL_DOCS_BATCH}, Conc:${FILL_DOCS_CONCURRENCY}), MaxRunMs: ${maxRunMs}`);

    // === Task 1: EXPAND (Keywords Expansion) ===
    const taskExpand = async () => {
        if (task === 'fill_docs') return null;

        // üöÄ Atomic Claim: Ìïú Î≤àÏùò DB Ìò∏Ï∂úÎ°ú Î∞∞ÏπòÎ•º ÏÑ†Ï†êÌïòÍ≥† Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¥ (is_expanded = 2 Processing)
        // Turso/SQLite 'UPDATE ... RETURNING' ÏßÄÏõê ÌôúÏö©
        // üöÄ ÏàòÏ†ï: is_expanded = 2 (Processing) ÏÉÅÌÉúÎ°ú ÎÇ®ÏùÄ ÌÇ§ÏõåÎìúÎèÑ Ïû¨Ï≤òÎ¶¨ ÎåÄÏÉÅÏóê Ìè¨Ìï®
        let seedsData: any[] = [];
        try {
            const claimResult = await db.execute({
                sql: `UPDATE keywords
                      SET is_expanded = 2
                      WHERE id IN (
                          SELECT id FROM keywords
                          WHERE (is_expanded = 0 OR is_expanded = 2) AND total_search_cnt >= ?
                          ORDER BY total_search_cnt DESC
                          LIMIT ?
                      )
                      RETURNING id, keyword, total_search_cnt`,
                args: [MIN_SEARCH_VOLUME, EXPAND_BATCH]
            });

            seedsData = claimResult.rows.map(row => ({
                id: row.id as string,
                keyword: row.keyword as string,
                total_search_cnt: row.total_search_cnt as number
            }));
        } catch (e: any) {
            console.error('[Batch] Expand Claim Failed:', e);
            return null;
        }

        if (!seedsData || seedsData.length === 0) return null;

        console.log(`[Batch] EXPAND: Claimed ${seedsData.length} seeds (Concurrency ${EXPAND_CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms)`);
        let stopDueToDeadline = false;

        const expandResults = await mapWithConcurrency(seedsData, EXPAND_CONCURRENCY, async (seed) => {
            // üöÄ ÌÑ∞Î≥¥Î™®Îìú: deadline Ï≤¥ÌÅ¨ ÏôÑÌôî (2500ms ‚Üí 1000ms)Î°ú Îçî ÎßéÏùÄ ÏãúÎìú Ï≤òÎ¶¨
            if (Date.now() > (deadline - 1000)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', seed };
            }

            try {
                const res = await processSeedKeyword(seed.keyword, 0, true, MIN_SEARCH_VOLUME);
                return { status: 'fulfilled', seed, saved: res.saved };
            } catch (e: any) {
                console.error(`[Batch] Seed Failed: ${seed.keyword} - ${e.message}`);
                return { status: 'rejected', seed, error: e.message };
            }
        });

        // ÌõÑÏ≤òÎ¶¨: ÏÑ±Í≥µ/Ïã§Ìå® ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ (Batch Update)
        const succeededIds = expandResults
            .filter(r => r.status === 'fulfilled')
            .map(r => r.seed.id);

        const failedIds = expandResults
            .filter(r => r.status === 'rejected')
            // Ïã§Ìå® Ïãú 0ÏúºÎ°ú ÎêòÎèåÎ†§ Ïû¨ÏãúÎèÑÌï†ÏßÄ, ÏïÑÎãàÎ©¥ 1(ÌôïÏû•ÏôÑÎ£å/Ïã§Ìå®)Î°ú Ï≤òÎ¶¨Ìï†ÏßÄ?
            // Î∞òÎ≥µÏ†ÅÏù∏ Ïã§Ìå® Î∞©ÏßÄÎ•º ÏúÑÌï¥ ÏùºÎã® 1(ÏôÑÎ£å Í∞ÑÏ£º)Î°ú Ï≤òÎ¶¨ÌïòÍ±∞ÎÇò 3(ÏóêÎü¨) Îì± Î≥ÑÎèÑ ÏÉÅÌÉúÍ∞Ä Ï¢ãÏúºÎÇò
            // Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ: is_expanded=1
            .map(r => r.seed.id);

        const allIdsToMarkDone = [...succeededIds, ...failedIds];

        // üöÄ ÏÉÅÌÉú ÏùºÍ¥Ñ ÏóÖÎç∞Ïù¥Ìä∏ (1Î≤àÏùò DB Ìò∏Ï∂ú)
        if (allIdsToMarkDone.length > 0) {
            try {
                // SQLite LIMIT on UPDATE is optional, standard UPDATE IN is safer
                const placeholders = allIdsToMarkDone.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET is_expanded = 1 WHERE id IN (${placeholders})`,
                    args: allIdsToMarkDone
                });
            } catch (e) {
                console.error('[Batch] Failed to mark seeds as expanded:', e);
            }
        }

        // is_expanded=2(Processing) ÏÉÅÌÉúÎ°ú ÎÇ®ÏùÄ(Îç∞ÎìúÎùºÏù∏ Ïä§ÌÇµ Îì±) Ìï≠Î™©Îì§ÏùÄ?
        // Îã§Ïùå Ïã§Ìñâ Ïãú ÏûêÎèôÏúºÎ°ú Ï≤òÎ¶¨ÎêòÍ±∞ÎÇò, 2 ÏÉÅÌÉúÏù∏Í±¥ Ïû¨ÏãúÎèÑ Î°úÏßÅ ÌïÑÏöî.
        // ÌòÑÏû¨ Î°úÏßÅÏÉÅ Ïä§ÌÇµÎêú Í±¥ Í∑∏ÎåÄÎ°ú 2Î°ú ÎÇ®Ïùå. 
        // Î°§Î∞± ÌïÑÏöî: Ïä§ÌÇµÎêú Ìï≠Î™©ÏùÄ 0ÏúºÎ°ú ÎêòÎèåÎ†§Ïïº Ìï®.
        const skippedIds = expandResults
            .filter(r => r.status === 'skipped_deadline')
            .map(r => r.seed.id);

        if (skippedIds.length > 0) {
            try {
                const placeholders = skippedIds.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET is_expanded = 0 WHERE id IN (${placeholders})`,
                    args: skippedIds
                });
            } catch (e) {
                console.error('[Batch] Failed to rollback skipped seeds:', e);
            }
        }

        const succeeded = expandResults.filter(r => r.status === 'fulfilled');
        return {
            processedSeeds: seedsData.length,
            totalSaved: succeeded.reduce((sum, r: any) => (sum + (r.saved || 0)), 0),
            stoppedDueToDeadline: stopDueToDeadline,
            details: expandResults.map((r: any) =>
                r.status === 'fulfilled' ? `${r.seed.keyword} (+${r.saved})` : `${r.seed.keyword} (${r.status})`
            )
        };
    };

    // === Task 2: FILL_DOCS (Document Counts) ===
    const taskFillDocs = async () => {
        if (task === 'expand') return null;

        const BATCH_SIZE = FILL_DOCS_BATCH;
        const CONCURRENCY = FILL_DOCS_CONCURRENCY;

        // üöÄ Atomic Claim: Î¨∏ÏÑú ÏàòÏßë ÎåÄÏÉÅ ÏÑ†Ï†ê (-2: Processing)
        let docsToFill: any[] = [];
        try {
            const claimResult = await db.execute({
                sql: `UPDATE keywords
                      SET total_doc_cnt = -2
                      WHERE id IN (
                          SELECT id FROM keywords
                          WHERE total_doc_cnt IS NULL
                          ORDER BY total_search_cnt DESC
                          LIMIT ?
                      )
                      RETURNING id, keyword, total_search_cnt`,
                args: [BATCH_SIZE]
            });

            docsToFill = claimResult.rows.map(row => ({
                id: row.id as string,
                keyword: row.keyword as string,
                total_search_cnt: row.total_search_cnt as number
            }));
        } catch (e: any) {
            console.error('[Batch] FillDocs Claim Failed:', e);
            return null;
        }

        if (!docsToFill || docsToFill.length === 0) return null;

        console.log(`[Batch] FILL_DOCS: Claimed ${docsToFill.length} items (Concurrency ${CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms)`);
        let stopDueToDeadline = false;

        const processedResults = await mapWithConcurrency(docsToFill, CONCURRENCY, async (item) => {
            // üöÄ ÌÑ∞Î≥¥Î™®Îìú: deadline Ï≤¥ÌÅ¨ ÏôÑÌôî (2500ms ‚Üí 1000ms)Î°ú Îçî ÎßéÏùÄ ÌÇ§ÏõåÎìú Ï≤òÎ¶¨
            if (Date.now() > (deadline - 1000)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', item };
            }
            try {
                const counts = await fetchDocumentCount(item.keyword);
                return { status: 'fulfilled', item, counts };
            } catch (e: any) {
                console.error(`[Batch] Error filling ${item.keyword}: ${e.message}`);
                return { status: 'rejected', keyword: item.keyword, error: e.message };
            }
        });

        const succeeded = processedResults.filter(r => r.status === 'fulfilled');
        const failed = processedResults.filter(r => r.status === 'rejected');

        // Ïä§ÌÇµÎêú Ìï≠Î™©ÏùÄ -2 -> NULLÎ°ú Î°§Î∞±Ìï¥Ïïº Îã§Ïãú Ïû°Ìûò
        const skipped = processedResults.filter(r => r.status === 'skipped_deadline');
        if (skipped.length > 0) {
            const skippedIds = skipped.map(r => r.item.id);
            try {
                const placeholders = skippedIds.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET total_doc_cnt = NULL WHERE id IN (${placeholders})`,
                    args: skippedIds
                });
            } catch (e) {
                console.error('[Batch] Error rolling back skipped docs:', e);
            }
        }

        // Success Updates (will overwrite -2 with real count)
        const successUpdates = succeeded.map((res: any) => {
            const { item, counts } = res;
            const viewDocCnt = (counts.blog || 0) + (counts.cafe || 0) + (counts.web || 0);
            let ratio = 0;
            let tier = 'UNRANKED';

            if (viewDocCnt > 0) {
                ratio = item.total_search_cnt / viewDocCnt;
                if (viewDocCnt <= 100 && ratio > 5) tier = '1Îì±Í∏â';
                else if (ratio > 10) tier = '2Îì±Í∏â';
                else if (ratio > 5) tier = '3Îì±Í∏â';
                else if (ratio > 1) tier = '4Îì±Í∏â';
                else tier = '5Îì±Í∏â';
            } else if (item.total_search_cnt > 0) {
                tier = '1Îì±Í∏â';
                ratio = 99.99;
            }

            return {
                id: item.id,
                keyword: item.keyword,
                total_search_cnt: item.total_search_cnt,
                total_doc_cnt: counts.total,
                blog_doc_cnt: counts.blog,
                cafe_doc_cnt: counts.cafe,
                web_doc_cnt: counts.web,
                news_doc_cnt: counts.news,
                golden_ratio: ratio,
                tier: tier,
                updated_at: new Date().toISOString()
            };
        });

        // Failure Updates (Error Flag: -1)
        const failureUpdates = failed.map((res: any) => {
            const { keyword, error } = res;
            const original = docsToFill.find(d => d.keyword === keyword);
            if (!original) return null;

            return {
                id: original.id,
                keyword: keyword,
                total_search_cnt: original.total_search_cnt || 0,
                total_doc_cnt: -1, // Error Flag
                blog_doc_cnt: 0,
                cafe_doc_cnt: 0,
                web_doc_cnt: 0,
                news_doc_cnt: 0,
                golden_ratio: 0,
                tier: 'ERROR',
                updated_at: new Date().toISOString()
            };
        }).filter((item): item is NonNullable<typeof item> => item !== null);

        const updates = [...successUpdates, ...failureUpdates];
        const now = getCurrentTimestamp();

        if (updates.length > 0) {
            let transactionStarted = false;
            try {
                await db.execute({ sql: 'BEGIN TRANSACTION' });
                transactionStarted = true;
                // üöÄ ÌÑ∞Î≥¥Î™®Îìú: Î∞∞Ïπò ÌÅ¨Í∏∞ ÎåÄÌè≠ Ï¶ùÍ∞Ä (200 ‚Üí 1000)Î°ú DB Ìò∏Ï∂ú ÏµúÏÜåÌôî
                const batchSize = 1000; // DB Ìò∏Ï∂ú ÌöüÏàò 80% Í∞êÏÜå
                for (let i = 0; i < updates.length; i += batchSize) {
                    const batch = updates.slice(i, i + batchSize);
                    const statements = batch.map(update => ({
                        sql: `INSERT OR REPLACE INTO keywords (
                            id, total_doc_cnt, blog_doc_cnt, cafe_doc_cnt,
                            web_doc_cnt, news_doc_cnt, golden_ratio, tier, updated_at
                        ) VALUES (
                            (SELECT id FROM keywords WHERE id = ?),
                            ?, ?, ?, ?, ?, ?, ?, ?
                        )`,
                        args: [
                            update.id,
                            update.total_doc_cnt,
                            update.blog_doc_cnt || 0,
                            update.cafe_doc_cnt || 0,
                            update.web_doc_cnt || 0,
                            update.news_doc_cnt || 0,
                            update.golden_ratio,
                            update.tier,
                            now
                        ]
                    }));

                    await db.batch(statements);
                }
                await db.execute({ sql: 'COMMIT' });
            } catch (upsertError: any) {
                // Only rollback if transaction was actually started
                if (transactionStarted) {
                    try {
                        await db.execute({ sql: 'ROLLBACK' });
                    } catch (rollbackError: any) {
                        // Ignore rollback errors (transaction might already be rolled back)
                        console.error('[Batch] Rollback error (ignored):', rollbackError.message);
                    }
                }
                console.error('[Batch] Transaction UPSERT Error:', upsertError);
                return {
                    processed: 0,
                    failed: docsToFill.length,
                    errors: [`Transaction UPSERT Failed: ${upsertError.message}`]
                };
            }
        }

        return {
            processed: successUpdates.length,
            failed: failed.length,
            stoppedDueToDeadline: stopDueToDeadline,
            errors: failed.slice(0, 3).map((f: any) => `${f.keyword}: ${f.error}`)
        };
    };


    try {
        // Execute Both Tasks in Parallel
        const [expandResult, fillDocsResult] = await Promise.all([
            taskExpand(),
            taskFillDocs()
        ]);

        const duration = ((Date.now() - start) / 1000).toFixed(1);
        console.log(`[Batch] Completed in ${duration}s`);

        return {
            success: true,
            mode,
            task,
            expand: expandResult,
            fillDocs: fillDocsResult
        };

    } catch (e: any) {
        console.error('Batch Error:', e);
        return { success: false, error: e.message };
    }
}

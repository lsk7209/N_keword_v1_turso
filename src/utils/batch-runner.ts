
import { getTursoClient, getCurrentTimestamp } from '@/utils/turso';
import { processSeedKeyword } from '@/utils/mining-engine';
import { fetchDocumentCount } from '@/utils/naver-api';

type MiningMode = 'NORMAL' | 'TURBO';
type MiningTask = 'all' | 'expand' | 'fill_docs';

export interface MiningBatchOptions {
    task?: MiningTask;
    mode?: MiningMode; // optional override
    seedCount?: number;
    expandBatch?: number;
    expandConcurrency?: number;
    fillDocsBatch?: number;
    fillDocsConcurrency?: number; // keywords concurrently fetching doc counts
    maxRunMs?: number; // hard deadline to avoid Vercel timeout (default: 55s)
    minSearchVolume?: number;
}

function clampInt(val: unknown, min: number, max: number, fallback: number) {
    const n = typeof val === 'number' ? val : Number(val);
    if (!Number.isFinite(n)) return fallback;
    return Math.max(min, Math.min(max, Math.trunc(n)));
}

async function mapWithConcurrency<T, R>(
    items: T[],
    concurrency: number,
    worker: (item: T, idx: number) => Promise<R>
): Promise<R[]> {
    const results: R[] = new Array(items.length);
    let nextIndex = 0;
    const workers = new Array(Math.max(1, concurrency)).fill(null).map(async () => {
        while (true) {
            const idx = nextIndex++;
            if (idx >= items.length) return;
            results[idx] = await worker(items[idx], idx);
        }
    });
    await Promise.all(workers);
    return results;
}

export async function runMiningBatch(options: MiningBatchOptions = {}) {
    const db = getTursoClient();

    // íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œê¹…
    const start = Date.now();
    console.log('[Batch] Starting Parallel Mining Batch...');

    // í„°ë³´ëª¨ë“œ í™•ì¸ (API í‚¤ ìµœëŒ€ í™œìš©ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì¡°ì •)
    const settingResult = await db.execute({
        sql: 'SELECT value FROM settings WHERE key = ?',
        args: ['mining_mode']
    });
    const setting = settingResult.rows.length > 0 ? { value: settingResult.rows[0].value } : null;

    // JSONB ê°’ íŒŒì‹± (getMiningModeì™€ ë™ì¼í•œ ë¡œì§)
    let mode: MiningMode = 'TURBO'; // ê¸°ë³¸ê°’ì€ TURBO
    if (setting) {
        const rawValue = (setting as any)?.value;
        if (typeof rawValue === 'string') {
            mode = rawValue.replace(/^"|"$/g, '').toUpperCase() as MiningMode;
        } else {
            mode = String(rawValue).toUpperCase() as MiningMode;
        }
        if (mode !== 'NORMAL' && mode !== 'TURBO') {
            mode = 'TURBO'; // ê¸°ë³¸ê°’ì€ TURBO
        }
    }
    if (options.mode === 'NORMAL' || options.mode === 'TURBO') {
        mode = options.mode;
    }
    const isTurboMode = mode === 'TURBO';

    const task: MiningTask = (options.task === 'expand' || options.task === 'fill_docs' || options.task === 'all')
        ? options.task
        : 'all';

    const maxRunMs = clampInt(options.maxRunMs, 10_000, 58_000, 55_000);
    const deadline = start + maxRunMs;

    // í„°ë³´ëª¨ë“œ: API í‚¤ ìµœëŒ€ í™œìš© (ê²€ìƒ‰ê´‘ê³  API 14ê°œ í™œìš©)
    // ì¼ë°˜ ëª¨ë“œ: ì•ˆì •ì ì¸ ìˆ˜ì§‘ (5ë¶„ë§ˆë‹¤ GitHub Actions)
    const SEED_COUNT = clampInt(options.seedCount, 0, 50, isTurboMode ? 20 : 5); // turbo default raised
    const EXPAND_BATCH = clampInt(options.expandBatch, 1, 300, isTurboMode ? 120 : 50); // í„°ë³´: 120ê°œ, ì¼ë°˜: 50ê°œ (14ê°œ í‚¤ í™œìš©)
    const EXPAND_CONCURRENCY = clampInt(options.expandConcurrency, 1, 20, isTurboMode ? 12 : 4); // í„°ë³´: 12ê°œ (14ê°œ í‚¤ í™œìš©), ì¼ë°˜: 4ê°œ
    const FILL_DOCS_BATCH = clampInt(options.fillDocsBatch, 1, 300, isTurboMode ? 180 : 100); // í„°ë³´: 180ê°œ (30ê°œ í‚¤ í™œìš©), ì¼ë°˜: 100ê°œ
    const FILL_DOCS_CONCURRENCY = clampInt(options.fillDocsConcurrency, 1, 40, isTurboMode ? 28 : 20); // í„°ë³´: 28ê°œ (30ê°œ í‚¤ í™œìš©), ì¼ë°˜: 20ê°œ
    // ìµœì†Œ ê²€ìƒ‰ëŸ‰ 1000 ê°•ì œ (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ 0ì´ ì „ë‹¬ë˜ì–´ë„ ìµœì†Œ 1000 ì ìš©)
    const MIN_SEARCH_VOLUME = Math.max(1000, clampInt(options.minSearchVolume, 0, 50_000, 1000));

    console.log(`[Batch] Mode: ${isTurboMode ? 'TURBO (Max API Usage)' : 'NORMAL'}, Task: ${task}, ExpandBatch: ${EXPAND_BATCH}, ExpandConcurrency: ${EXPAND_CONCURRENCY}, FillDocs: ${FILL_DOCS_BATCH}, FillConcurrency: ${FILL_DOCS_CONCURRENCY}, MaxRunMs: ${maxRunMs}`);

    // === Task 1: EXPAND (Keywords Expansion) ===
    const taskExpand = async () => {
        if (task === 'fill_docs') return null;

        // ğŸš€ ì¸ë±ìŠ¤ í™œìš© ìµœì í™”: is_expanded = 0 ì¡°ê±´ì— ëŒ€í•œ íš¨ìœ¨ì  ì¡°íšŒ
        const seedsResult = await db.execute({
            sql: `SELECT id, keyword, total_search_cnt FROM keywords
                  WHERE is_expanded = 0 AND total_search_cnt >= ?
                  ORDER BY total_search_cnt DESC
                  LIMIT ?`,
            args: [MIN_SEARCH_VOLUME, isTurboMode ? 500 : 200]
        });

        const seedsData = seedsResult.rows.map(row => ({
            id: row.id as string,
            keyword: row.keyword as string,
            total_search_cnt: row.total_search_cnt as number
        }));

        if (!seedsData || seedsData.length === 0) return null;

        // ê²€ìƒ‰ëŸ‰ ìƒìœ„ ìš°ì„  ì„ íƒ (ëœë¤ ëŒ€ì‹ )
        const seeds = seedsData.slice(0, EXPAND_BATCH);

        console.log(`[Batch] EXPAND: Processing up to ${seeds.length} seeds (Concurrency ${EXPAND_CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms, min: ${MIN_SEARCH_VOLUME})`);
        let stopDueToDeadline = false;

        const expandResults = await mapWithConcurrency(seeds, EXPAND_CONCURRENCY, async (seed) => {
            if (Date.now() > (deadline - 2500)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', seed: seed.keyword };
            }

            // Optimistic lock (best-effort): claim the seed so parallel invocations don't duplicate work
            const lockResult = await db.execute({
                sql: 'UPDATE keywords SET is_expanded = 1 WHERE id = ? AND is_expanded = 0',
                args: [seed.id]
            });

            if (lockResult.rowsAffected === 0) return { status: 'skipped', seed: seed.keyword };

            try {
                const res = await processSeedKeyword(seed.keyword, 0, true, MIN_SEARCH_VOLUME);
                // Mark confirmed
                await db.execute({
                    sql: 'UPDATE keywords SET is_expanded = 1 WHERE id = ?',
                    args: [seed.id]
                });
                return { status: 'fulfilled', seed: seed.keyword, saved: res.saved };
            } catch (e: any) {
                console.error(`[Batch] Seed Failed: ${seed.keyword} - ${e.message}`);
                await db.execute({
                    sql: 'UPDATE keywords SET is_expanded = 1 WHERE id = ?',
                    args: [seed.id]
                });
                return { status: 'rejected', seed: seed.keyword, error: e.message };
            }
        });

        const succeeded = expandResults.filter(r => r.status === 'fulfilled');
        return {
            processedSeeds: seeds.length,
            totalSaved: succeeded.reduce((sum, r: any) => (sum + (r.saved || 0)), 0),
            stoppedDueToDeadline: stopDueToDeadline,
            details: expandResults.map((r: any) =>
                r.status === 'fulfilled' ? `${r.seed} (+${r.saved})` : `${r.seed} (${r.status})`
            )
        };
    };

    // === Task 2: FILL_DOCS (Document Counts) ===
    const taskFillDocs = async () => {
        if (task === 'expand') return null;

        // í„°ë³´ëª¨ë“œ: API í‚¤ ìµœëŒ€ í™œìš©ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        const BATCH_SIZE = FILL_DOCS_BATCH;
        const CONCURRENCY = FILL_DOCS_CONCURRENCY;

        // ğŸš€ ì¸ë±ìŠ¤ í™œìš© ìµœì í™”: total_doc_cnt IS NULL ì¡°ê±´ì— ëŒ€í•œ íš¨ìœ¨ì  ì¡°íšŒ
        const docsResult = await db.execute({
            sql: `SELECT id, keyword, total_search_cnt FROM keywords
                  WHERE total_doc_cnt IS NULL
                  ORDER BY total_search_cnt DESC
                  LIMIT ?`,
            args: [BATCH_SIZE]
        });

        const docsToFill = docsResult.rows.map(row => ({
            id: row.id as string,
            keyword: row.keyword as string,
            total_search_cnt: row.total_search_cnt as number
        }));

        if (!docsToFill || docsToFill.length === 0) return null;

        console.log(`[Batch] FILL_DOCS: Processing ${docsToFill.length} items (Concurrency ${CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms)`);
        let stopDueToDeadline = false;

        const processedResults = await mapWithConcurrency(docsToFill, CONCURRENCY, async (item) => {
            // Keep a safety margin to avoid Vercel hard timeout.
            if (Date.now() > (deadline - 2500)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', item };
            }
            try {
                const counts = await fetchDocumentCount(item.keyword);
                return { status: 'fulfilled', item, counts };
            } catch (e: any) {
                console.error(`[Batch] Error filling ${item.keyword}: ${e.message}`);
                return { status: 'rejected', keyword: item.keyword, error: e.message };
            }
        });

        const succeeded = processedResults.filter(r => r.status === 'fulfilled');
        const failed = processedResults.filter(r => r.status === 'rejected');

        // Success Updates
        const successUpdates = succeeded.map((res: any) => {
            const { item, counts } = res;
            const viewDocCnt = (counts.blog || 0) + (counts.cafe || 0) + (counts.web || 0);
            let ratio = 0;
            let tier = 'UNRANKED';

            if (viewDocCnt > 0) {
                ratio = item.total_search_cnt / viewDocCnt;
                if (viewDocCnt <= 100 && ratio > 5) tier = '1ë“±ê¸‰';
                else if (ratio > 10) tier = '2ë“±ê¸‰';
                else if (ratio > 5) tier = '3ë“±ê¸‰';
                else if (ratio > 1) tier = '4ë“±ê¸‰';
                else tier = '5ë“±ê¸‰';
            } else if (item.total_search_cnt > 0) {
                tier = '1ë“±ê¸‰';
                ratio = 99.99;
            }

            return {
                id: item.id,
                keyword: item.keyword,
                total_search_cnt: item.total_search_cnt,
                total_doc_cnt: counts.total,
                blog_doc_cnt: counts.blog,
                cafe_doc_cnt: counts.cafe,
                web_doc_cnt: counts.web,
                news_doc_cnt: counts.news,
                golden_ratio: ratio,
                tier: tier,
                updated_at: new Date().toISOString()
            };
        });

        // Failure Updates (Mark as Error to prevent reuse)
        const failureUpdates = failed.map((res: any) => {
            const { keyword, error } = res;
            // Find original item ID from docsToFill
            const original = docsToFill.find(d => d.keyword === keyword);
            if (!original) return null;

            return {
                id: original.id,
                keyword: keyword,
                total_search_cnt: original.total_search_cnt || 0,
                total_doc_cnt: -1, // Error Flag
                blog_doc_cnt: 0,
                cafe_doc_cnt: 0,
                web_doc_cnt: 0,
                news_doc_cnt: 0,
                golden_ratio: 0,
                tier: 'ERROR',
                updated_at: new Date().toISOString()
            };
        }).filter((item): item is NonNullable<typeof item> => item !== null);

        const updates = [...successUpdates, ...failureUpdates];
        const now = getCurrentTimestamp();

        if (updates.length > 0) {
            try {
                // ğŸš€ íŠ¸ëœì­ì…˜ UPSERT: ëª¨ë“  ì—…ë°ì´íŠ¸ë¥¼ ë‹¨ì¼ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬, DB í˜¸ì¶œ ìµœì†Œí™”
                await db.execute({ sql: 'BEGIN TRANSACTION' });

                const batchSize = 200; // 50 â†’ 200, 4ë°° ì¦ê°€
                for (let i = 0; i < updates.length; i += batchSize) {
                    const batch = updates.slice(i, i + batchSize);
                    const statements = batch.map(update => ({
                        sql: `INSERT OR REPLACE INTO keywords (
                            id, total_doc_cnt, blog_doc_cnt, cafe_doc_cnt,
                            web_doc_cnt, news_doc_cnt, golden_ratio, tier, updated_at
                        ) VALUES (
                            (SELECT id FROM keywords WHERE id = ?),
                            ?, ?, ?, ?, ?, ?, ?, ?
                        )`,
                        args: [
                            update.id,
                            update.total_doc_cnt,
                            update.blog_doc_cnt || 0,
                            update.cafe_doc_cnt || 0,
                            update.web_doc_cnt || 0,
                            update.news_doc_cnt || 0,
                            update.golden_ratio,
                            update.tier,
                            now
                        ]
                    }));

                    await db.batch(statements);
                }

                await db.execute({ sql: 'COMMIT' });
            } catch (upsertError: any) {
                await db.execute({ sql: 'ROLLBACK' });
                console.error('[Batch] Transaction UPSERT Error:', upsertError);
                return {
                    processed: 0,
                    failed: docsToFill.length,
                    errors: [`Transaction UPSERT Failed: ${upsertError.message}`]
                };
            }
        }

        return {
            processed: successUpdates.length,
            failed: failed.length,
            stoppedDueToDeadline: stopDueToDeadline,
            errors: failed.slice(0, 3).map((f: any) => `${f.keyword}: ${f.error}`)
        };
    };

    try {
        // Execute Both Tasks in Parallel
        const [expandResult, fillDocsResult] = await Promise.all([
            taskExpand(),
            taskFillDocs()
        ]);

        const duration = ((Date.now() - start) / 1000).toFixed(1);
        console.log(`[Batch] Completed in ${duration}s`);

        return {
            success: true,
            mode,
            task,
            expand: expandResult,
            fillDocs: fillDocsResult
        };

    } catch (e: any) {
        console.error('Batch Error:', e);
        return { success: false, error: e.message };
    }
}

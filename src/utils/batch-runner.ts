
import { getTursoClient, getCurrentTimestamp } from '@/utils/turso';
import { processSeedKeyword } from '@/utils/mining-engine';
import { fetchDocumentCount } from '@/utils/naver-api';
import { keyManager } from '@/utils/key-manager';

type MiningMode = 'NORMAL' | 'TURBO';
type MiningTask = 'all' | 'expand' | 'fill_docs';

export interface MiningBatchOptions {
    task?: MiningTask;
    mode?: MiningMode; // optional override
    seedCount?: number;
    expandBatch?: number;
    expandConcurrency?: number;
    fillDocsBatch?: number;
    fillDocsConcurrency?: number; // keywords concurrently fetching doc counts
    maxRunMs?: number; // hard deadline to avoid Vercel timeout (default: 55s)
    minSearchVolume?: number;
}

function clampInt(val: unknown, min: number, max: number, fallback: number) {
    const n = typeof val === 'number' ? val : Number(val);
    if (!Number.isFinite(n)) return fallback;
    return Math.max(min, Math.min(max, Math.trunc(n)));
}

async function mapWithConcurrency<T, R>(
    items: T[],
    concurrency: number,
    worker: (item: T, idx: number) => Promise<R>
): Promise<R[]> {
    const results: R[] = new Array(items.length);
    let nextIndex = 0;
    const workers = new Array(Math.max(1, concurrency)).fill(null).map(async () => {
        while (true) {
            const idx = nextIndex++;
            if (idx >= items.length) return;
            results[idx] = await worker(items[idx], idx);
        }
    });
    await Promise.all(workers);
    return results;
}

export async function runMiningBatch(options: MiningBatchOptions = {}) {
    const db = getTursoClient();

    // íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œê¹…
    const start = Date.now();
    console.log('[Batch] Starting Parallel Mining Batch...');

    // ğŸš€ í„°ë³´ëª¨ë“œ: DB ì½ê¸° ìµœì†Œí™” - options.mode ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ TURBO ì‚¬ìš©
    // settings í…Œì´ë¸” ì¡°íšŒëŠ” ì„ íƒì ìœ¼ë¡œë§Œ ìˆ˜í–‰ (DB ì½ê¸° 1íšŒ ì ˆì•½)
    let mode: MiningMode = 'TURBO'; // ê¸°ë³¸ê°’ì€ TURBO (ëŒ€ëŸ‰ ìˆ˜ì§‘ ìµœì í™”)
    let isTurboMode = true;

    if (options.mode === 'NORMAL' || options.mode === 'TURBO') {
        mode = options.mode;
        isTurboMode = mode === 'TURBO';
    } else {
        // options.modeê°€ ì—†ì„ ë•Œë§Œ DB ì¡°íšŒ (ìµœì†Œí™”)
        try {
            const settingResult = await db.execute({
                sql: 'SELECT value FROM settings WHERE key = ?',
                args: ['mining_mode']
            });
            if (settingResult.rows.length > 0) {
                const rawValue = (settingResult.rows[0] as any).value;
                if (typeof rawValue === 'string') {
                    mode = rawValue.replace(/^"|"$/g, '').toUpperCase() as MiningMode;
                } else {
                    mode = String(rawValue).toUpperCase() as MiningMode;
                }
                if (mode !== 'NORMAL' && mode !== 'TURBO') {
                    mode = 'TURBO';
                }
                isTurboMode = mode === 'TURBO';
            }
        } catch (e) {
            // DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ TURBO ì‚¬ìš©
            console.warn('[Batch] Failed to read mining_mode from DB, using TURBO default');
        }
    }

    const task: MiningTask = (options.task === 'expand' || options.task === 'fill_docs' || options.task === 'all')
        ? options.task
        : 'all';

    // ğŸš€ í„°ë³´ëª¨ë“œ: ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ í™•ëŒ€ (55ì´ˆ â†’ 58ì´ˆ)ë¡œ ë” ë§ì€ ì²˜ë¦¬
    const maxRunMs = clampInt(options.maxRunMs, 10_000, 58_000, 58_000);
    const deadline = start + maxRunMs;

    // í„°ë³´ëª¨ë“œ: API í‚¤ ìˆ˜ì— ë”°ë¥¸ ë™ì  í™•ì¥ (Aggressive)
    const searchKeyCount = keyManager.getKeyCount('SEARCH');
    const adKeyCount = keyManager.getKeyCount('AD');

    // ğŸš€ í„°ë³´ëª¨ë“œ: ìµœëŒ€ ì„±ëŠ¥ì„ ìœ„í•œ ê³µê²©ì  ì„¤ì • (API í‚¤ ìµœëŒ€ í™œìš©)
    // AD Key: ê°œë‹¹ 8-10ë°° (í„°ë³´ëª¨ë“œì—ì„œëŠ” ìµœëŒ€í•œ í™œìš©)
    // ìµœì†Œ 20ê°œ, í‚¤ê°€ ë§ì„ìˆ˜ë¡ ì¦ê°€ (ìµœëŒ€ ì œí•œ ì—†ìŒ)
    let baseExpandConcurrency = isTurboMode
        ? Math.max(20, adKeyCount * 10)  // í„°ë³´: í‚¤ë‹¹ 10ë°°, ìµœì†Œ 20 (5ë°° â†’ 10ë°°ë¡œ ì¦ê°€)
        : Math.max(4, adKeyCount * 2);  // ì¼ë°˜: í‚¤ë‹¹ 2ë°°, ìµœì†Œ 4

    // Search Key: ê°œë‹¹ 10-12ë°° (í„°ë³´ëª¨ë“œì—ì„œëŠ” ìµœëŒ€í•œ í™œìš©)
    // ìµœì†Œ 50ê°œ, í‚¤ê°€ ë§ì„ìˆ˜ë¡ ì¦ê°€ (ìµœëŒ€ ì œí•œ ì—†ìŒ)
    let baseFillConcurrency = isTurboMode
        ? Math.max(50, searchKeyCount * 12)  // í„°ë³´: í‚¤ë‹¹ 12ë°°, ìµœì†Œ 50 (6ë°° â†’ 12ë°°ë¡œ ì¦ê°€)
        : Math.max(20, searchKeyCount * 3); // ì¼ë°˜: í‚¤ë‹¹ 3ë°°, ìµœì†Œ 20

    console.log(`[Batch] ğŸš€ TURBO Mode: Key-based concurrency: AD keys=${adKeyCount} â†’ expand=${baseExpandConcurrency}, SEARCH keys=${searchKeyCount} â†’ fill=${baseFillConcurrency}`);

    const SEED_COUNT = clampInt(options.seedCount, 0, 50, isTurboMode ? 20 : 5);

    // ğŸš€ í„°ë³´ëª¨ë“œ: ë™ì‹œì„± ì œí•œì„ í¬ê²Œ í™•ëŒ€ (API í‚¤ ìµœëŒ€ í™œìš©)
    // EXPAND: ìµœëŒ€ 500ê¹Œì§€ í—ˆìš© (í„°ë³´ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬)
    const EXPAND_CONCURRENCY = clampInt(options.expandConcurrency, 1, isTurboMode ? 500 : 100, baseExpandConcurrency);
    // FILL_DOCS: ìµœëŒ€ 1000ê¹Œì§€ í—ˆìš© (í„°ë³´ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬)
    const FILL_DOCS_CONCURRENCY = clampInt(options.fillDocsConcurrency, 1, isTurboMode ? 1000 : 400, baseFillConcurrency);

    // ğŸš€ í„°ë³´ëª¨ë“œ: ë°°ì¹˜ í¬ê¸°ë¥¼ ìµœëŒ€í•œ í¬ê²Œ ì„¤ì • (API í‚¤ ìµœëŒ€ í™œìš©)
    // EXPAND: ë™ì‹œì„±ì˜ 15-20ë°° (í„°ë³´ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ì‹œë“œ ì²˜ë¦¬)
    const expandBatchBase = isTurboMode
        ? Math.max(200, baseExpandConcurrency * 20)  // í„°ë³´: 20ë°°, ìµœì†Œ 200 (12ë°° â†’ 20ë°°ë¡œ ì¦ê°€)
        : Math.max(50, baseExpandConcurrency * 8);   // ì¼ë°˜: 8ë°°, ìµœì†Œ 50

    // FILL_DOCS: ë™ì‹œì„±ì˜ 15-20ë°° (í„°ë³´ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ í‚¤ì›Œë“œ ì²˜ë¦¬)
    const fillDocsBatchBase = isTurboMode
        ? Math.max(500, baseFillConcurrency * 20)  // í„°ë³´: 20ë°°, ìµœì†Œ 500 (10ë°° â†’ 20ë°°ë¡œ ì¦ê°€)
        : Math.max(100, baseFillConcurrency * 5);  // ì¼ë°˜: 5ë°°, ìµœì†Œ 100

    // ğŸš€ í„°ë³´ëª¨ë“œ: ë°°ì¹˜ í¬ê¸° ì œí•œì„ í¬ê²Œ í™•ëŒ€ (API í‚¤ ìµœëŒ€ í™œìš©)
    const EXPAND_BATCH = clampInt(options.expandBatch, 1, isTurboMode ? 5000 : 1000, expandBatchBase);
    const FILL_DOCS_BATCH = clampInt(options.fillDocsBatch, 1, isTurboMode ? 20000 : 5000, fillDocsBatchBase);

    // ìµœì†Œ ê²€ìƒ‰ëŸ‰ 100 ê°•ì œ (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ 0ì´ ì „ë‹¬ë˜ì–´ë„ ìµœì†Œ 100 ì ìš©)
    const MIN_SEARCH_VOLUME = Math.max(100, clampInt(options.minSearchVolume, 0, 50_000, 100));

    console.log(`[Batch] Mode: ${isTurboMode ? 'TURBO' : 'NORMAL'}, Keys(S/A): ${searchKeyCount}/${adKeyCount}, Task: ${task}`);
    console.log(`[Batch] Config: Expand(Batch:${EXPAND_BATCH}, Conc:${EXPAND_CONCURRENCY}), FillDocs(Batch:${FILL_DOCS_BATCH}, Conc:${FILL_DOCS_CONCURRENCY}), MaxRunMs: ${maxRunMs}`);

    // === Task 1: EXPAND (Keywords Expansion) ===
    const taskExpand = async () => {
        if (task === 'fill_docs') return null;

        // ğŸš€ Atomic Claim: í•œ ë²ˆì˜ DB í˜¸ì¶œë¡œ ë°°ì¹˜ë¥¼ ì„ ì í•˜ê³  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ (is_expanded = 2 Processing)
        // Turso/SQLite 'UPDATE ... RETURNING' ì§€ì› í™œìš©
        // ğŸš€ íš¨ìœ¨ì  í™•ì¥ ì „ëµ:
        // 1ìˆœìœ„: ë¯¸í™•ì¥ í‚¤ì›Œë“œ (is_expanded = 0) - ìƒˆë¡œìš´ í‚¤ì›Œë“œ ë°œêµ´
        // 2ìˆœìœ„: Processing ìƒíƒœ (is_expanded = 2) - ì´ì „ ì‹¤í–‰ ì¤‘ë‹¨ ê±´ ì¬ì‹œë„
        // 3ìˆœìœ„: 7ì¼ ì´ìƒ ê²½ê³¼ (is_expanded = 1 AND updated_at < 7 days) - íŠ¸ë Œë“œ ë³€í™” ë°˜ì˜
        let seedsData: any[] = [];
        try {
            const claimResult = await db.execute({
                sql: `UPDATE keywords
                      SET is_expanded = 2
                      WHERE id IN (
                          SELECT id FROM keywords
                          WHERE (
                            is_expanded = 0 
                            OR is_expanded = 2
                            OR (is_expanded = 1 AND updated_at < datetime('now', '-7 days'))
                          ) 
                          AND total_search_cnt >= ?
                          ORDER BY 
                            CASE 
                              WHEN is_expanded = 0 THEN 0
                              WHEN is_expanded = 2 THEN 1
                              ELSE 2
                            END,
                            total_search_cnt DESC
                          LIMIT ?
                      )
                      RETURNING id, keyword, total_search_cnt`,
                args: [MIN_SEARCH_VOLUME, EXPAND_BATCH]
            });

            seedsData = claimResult.rows.map(row => ({
                id: row.id as string,
                keyword: row.keyword as string,
                total_search_cnt: row.total_search_cnt as number
            }));
        } catch (e: any) {
            console.error('[Batch] Expand Claim Failed:', e);
            return null;
        }

        if (!seedsData || seedsData.length === 0) return null;

        console.log(`[Batch] EXPAND: Claimed ${seedsData.length} seeds (Concurrency ${EXPAND_CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms)`);
        let stopDueToDeadline = false;

        const expandResults = await mapWithConcurrency(seedsData, EXPAND_CONCURRENCY, async (seed) => {
            // ğŸš€ í„°ë³´ëª¨ë“œ: deadline ì²´í¬ ì™„í™” (2500ms â†’ 1000ms)ë¡œ ë” ë§ì€ ì‹œë“œ ì²˜ë¦¬
            if (Date.now() > (deadline - 1000)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', seed };
            }

            try {
                const res = await processSeedKeyword(seed.keyword, 0, true, MIN_SEARCH_VOLUME);
                if (res.saved === 0) {
                    console.warn(`[Batch] âš ï¸ Seed "${seed.keyword}" processed but saved 0 keywords (processed: ${res.processed})`);
                }
                return { status: 'fulfilled', seed, saved: res.saved };
            } catch (e: any) {
                console.error(`[Batch] âŒ Seed Failed: ${seed.keyword} - ${e.message}`, {
                    stack: e.stack,
                    name: e.name,
                    code: e.code
                });
                return { status: 'rejected', seed, error: e.message };
            }
        });

        // í›„ì²˜ë¦¬: ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ (Batch Update)
        const succeededIds = expandResults
            .filter(r => r.status === 'fulfilled')
            .map(r => r.seed.id);

        const failedIds = expandResults
            .filter(r => r.status === 'rejected')
            // ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ë˜ëŒë ¤ ì¬ì‹œë„í• ì§€, ì•„ë‹ˆë©´ 1(í™•ì¥ì™„ë£Œ/ì‹¤íŒ¨)ë¡œ ì²˜ë¦¬í• ì§€?
            // ë°˜ë³µì ì¸ ì‹¤íŒ¨ ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë‹¨ 1(ì™„ë£Œ ê°„ì£¼)ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ 3(ì—ëŸ¬) ë“± ë³„ë„ ìƒíƒœê°€ ì¢‹ìœ¼ë‚˜
            // ê¸°ì¡´ ë¡œì§ ìœ ì§€: is_expanded=1
            .map(r => r.seed.id);

        const allIdsToMarkDone = [...succeededIds, ...failedIds];

        // ğŸš€ ìƒíƒœ ì¼ê´„ ì—…ë°ì´íŠ¸ (1ë²ˆì˜ DB í˜¸ì¶œ)
        if (allIdsToMarkDone.length > 0) {
            try {
                // SQLite LIMIT on UPDATE is optional, standard UPDATE IN is safer
                const placeholders = allIdsToMarkDone.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET is_expanded = 1 WHERE id IN (${placeholders})`,
                    args: allIdsToMarkDone
                });
            } catch (e) {
                console.error('[Batch] Failed to mark seeds as expanded:', e);
            }
        }

        // is_expanded=2(Processing) ìƒíƒœë¡œ ë‚¨ì€(ë°ë“œë¼ì¸ ìŠ¤í‚µ ë“±) í•­ëª©ë“¤ì€?
        // ë‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ê±°ë‚˜, 2 ìƒíƒœì¸ê±´ ì¬ì‹œë„ ë¡œì§ í•„ìš”.
        // í˜„ì¬ ë¡œì§ìƒ ìŠ¤í‚µëœ ê±´ ê·¸ëŒ€ë¡œ 2ë¡œ ë‚¨ìŒ. 
        // ë¡¤ë°± í•„ìš”: ìŠ¤í‚µëœ í•­ëª©ì€ 0ìœ¼ë¡œ ë˜ëŒë ¤ì•¼ í•¨.
        const skippedIds = expandResults
            .filter(r => r.status === 'skipped_deadline')
            .map(r => r.seed.id);

        if (skippedIds.length > 0) {
            try {
                const placeholders = skippedIds.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET is_expanded = 0 WHERE id IN (${placeholders})`,
                    args: skippedIds
                });
            } catch (e) {
                console.error('[Batch] Failed to rollback skipped seeds:', e);
            }
        }

        const succeeded = expandResults.filter(r => r.status === 'fulfilled');
        return {
            processedSeeds: seedsData.length,
            totalSaved: succeeded.reduce((sum, r: any) => (sum + (r.saved || 0)), 0),
            stoppedDueToDeadline: stopDueToDeadline,
            details: expandResults.map((r: any) =>
                r.status === 'fulfilled' ? `${r.seed.keyword} (+${r.saved})` : `${r.seed.keyword} (${r.status})`
            )
        };
    };

    // === Task 2: FILL_DOCS (Document Counts) ===
    const taskFillDocs = async () => {
        if (task === 'expand') return null;

        const BATCH_SIZE = FILL_DOCS_BATCH;
        const CONCURRENCY = FILL_DOCS_CONCURRENCY;

        // ğŸš€ Atomic Claim: ë¬¸ì„œ ìˆ˜ì§‘ ëŒ€ìƒ ì„ ì  (-2: Processing)
        let docsToFill: any[] = [];
        try {
            const claimResult = await db.execute({
                sql: `UPDATE keywords
                      SET total_doc_cnt = -2
                      WHERE id IN (
                          SELECT id FROM keywords
                          WHERE total_doc_cnt IS NULL
                          ORDER BY total_search_cnt DESC
                          LIMIT ?
                      )
                      RETURNING id, keyword, total_search_cnt`,
                args: [BATCH_SIZE]
            });

            docsToFill = claimResult.rows.map(row => ({
                id: row.id as string,
                keyword: row.keyword as string,
                total_search_cnt: row.total_search_cnt as number
            }));
        } catch (e: any) {
            console.error('[Batch] FillDocs Claim Failed:', e);
            return null;
        }

        if (!docsToFill || docsToFill.length === 0) return null;

        console.log(`[Batch] FILL_DOCS: Claimed ${docsToFill.length} items (Concurrency ${CONCURRENCY}, Deadline in ${(deadline - Date.now())}ms)`);
        let stopDueToDeadline = false;

        const processedResults = await mapWithConcurrency(docsToFill, CONCURRENCY, async (item) => {
            // ğŸš€ í„°ë³´ëª¨ë“œ: deadline ì²´í¬ ì™„í™” (2500ms â†’ 1000ms)ë¡œ ë” ë§ì€ í‚¤ì›Œë“œ ì²˜ë¦¬
            if (Date.now() > (deadline - 1000)) {
                stopDueToDeadline = true;
                return { status: 'skipped_deadline', item };
            }
            try {
                const counts = await fetchDocumentCount(item.keyword);
                return { status: 'fulfilled', item, counts };
            } catch (e: any) {
                console.error(`[Batch] Error filling ${item.keyword}: ${e.message}`);
                return { status: 'rejected', keyword: item.keyword, error: e.message };
            }
        });

        const succeeded = processedResults.filter(r => r.status === 'fulfilled');
        const failed = processedResults.filter(r => r.status === 'rejected');

        // ìŠ¤í‚µëœ í•­ëª©ì€ -2 -> NULLë¡œ ë¡¤ë°±í•´ì•¼ ë‹¤ì‹œ ì¡í˜
        const skipped = processedResults.filter(r => r.status === 'skipped_deadline');
        if (skipped.length > 0) {
            const skippedIds = skipped.map(r => r.item.id);
            try {
                const placeholders = skippedIds.map(() => '?').join(',');
                await db.execute({
                    sql: `UPDATE keywords SET total_doc_cnt = NULL WHERE id IN (${placeholders})`,
                    args: skippedIds
                });
            } catch (e) {
                console.error('[Batch] Error rolling back skipped docs:', e);
            }
        }

        // Success Updates (will overwrite -2 with real count)
        const successUpdates = succeeded.map((res: any) => {
            const { item, counts } = res;
            const viewDocCnt = (counts.blog || 0) + (counts.cafe || 0) + (counts.web || 0);
            let ratio = 0;
            let tier = 'UNRANKED';

            if (viewDocCnt > 0) {
                ratio = item.total_search_cnt / viewDocCnt;
                if (viewDocCnt <= 100 && ratio > 5) tier = '1ë“±ê¸‰';
                else if (ratio > 10) tier = '2ë“±ê¸‰';
                else if (ratio > 5) tier = '3ë“±ê¸‰';
                else if (ratio > 1) tier = '4ë“±ê¸‰';
                else tier = '5ë“±ê¸‰';
            } else if (item.total_search_cnt > 0) {
                tier = '1ë“±ê¸‰';
                ratio = 99.99;
            }

            return {
                id: item.id,
                keyword: item.keyword,
                total_search_cnt: item.total_search_cnt,
                total_doc_cnt: counts.total,
                blog_doc_cnt: counts.blog,
                cafe_doc_cnt: counts.cafe,
                web_doc_cnt: counts.web,
                news_doc_cnt: counts.news,
                golden_ratio: ratio,
                tier: tier,
                updated_at: new Date().toISOString()
            };
        });

        // Failure Updates (Error Flag: -1)
        const failureUpdates = failed.map((res: any) => {
            const { keyword, error } = res;
            const original = docsToFill.find(d => d.keyword === keyword);
            if (!original) return null;

            return {
                id: original.id,
                keyword: keyword,
                total_search_cnt: original.total_search_cnt || 0,
                total_doc_cnt: -1, // Error Flag
                blog_doc_cnt: 0,
                cafe_doc_cnt: 0,
                web_doc_cnt: 0,
                news_doc_cnt: 0,
                golden_ratio: 0,
                tier: 'ERROR',
                updated_at: new Date().toISOString()
            };
        }).filter((item): item is NonNullable<typeof item> => item !== null);

        const updates = [...successUpdates, ...failureUpdates];
        const now = getCurrentTimestamp();

        if (updates.length > 0) {
            try {
                // ğŸš€ FIX: db.batch()ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìì²´ íŠ¸ëœì­ì…˜ì„ ê´€ë¦¬í•˜ë¯€ë¡œ BEGIN/COMMIT ë¶ˆí•„ìš”
                // Turso/libsqlì˜ db.batch()ëŠ” ìë™ìœ¼ë¡œ íŠ¸ëœì­ì…˜ì„ ì‹œì‘í•˜ê³  ì»¤ë°‹í•©ë‹ˆë‹¤.
                // ì™¸ë¶€ì—ì„œ BEGIN/COMMITì„ ì‚¬ìš©í•˜ë©´ ì¶©ëŒì´ ë°œìƒí•˜ì—¬ "cannot commit - no transaction is active" ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.

                // ğŸš€ í„°ë³´ëª¨ë“œ: ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€ (200 â†’ 1000)ë¡œ DB í˜¸ì¶œ ìµœì†Œí™”
                const batchSize = 1000; // DB í˜¸ì¶œ íšŸìˆ˜ 80% ê°ì†Œ

                for (let i = 0; i < updates.length; i += batchSize) {
                    const batch = updates.slice(i, i + batchSize);
                    const statements = batch.map(update => ({
                        sql: `INSERT OR REPLACE INTO keywords (
                            id, total_doc_cnt, blog_doc_cnt, cafe_doc_cnt,
                            web_doc_cnt, news_doc_cnt, golden_ratio, tier, updated_at
                        ) VALUES (
                            (SELECT id FROM keywords WHERE id = ?),
                            ?, ?, ?, ?, ?, ?, ?, ?
                        )`,
                        args: [
                            update.id,
                            update.total_doc_cnt,
                            update.blog_doc_cnt || 0,
                            update.cafe_doc_cnt || 0,
                            update.web_doc_cnt || 0,
                            update.news_doc_cnt || 0,
                            update.golden_ratio,
                            update.tier,
                            now
                        ]
                    }));

                    await db.batch(statements);
                }
            } catch (upsertError: any) {
                console.error('[Batch] DB Batch Error:', {
                    message: upsertError.message,
                    stack: upsertError.stack,
                    name: upsertError.name,
                    code: upsertError.code,
                    updatesCount: updates.length
                });
                return {
                    processed: 0,
                    failed: docsToFill.length,
                    errors: [`Transaction UPSERT Failed: ${upsertError.message}`]
                };
            }
        }

        return {
            processed: successUpdates.length,
            failed: failed.length,
            stoppedDueToDeadline: stopDueToDeadline,
            errors: failed.slice(0, 3).map((f: any) => `${f.keyword}: ${f.error}`)
        };
    };


    try {
        // Execute Both Tasks in Parallel
        const [expandResult, fillDocsResult] = await Promise.all([
            taskExpand(),
            taskFillDocs()
        ]);

        const duration = ((Date.now() - start) / 1000).toFixed(1);
        console.log(`[Batch] Completed in ${duration}s`);

        return {
            success: true,
            mode,
            task,
            expand: expandResult,
            fillDocs: fillDocsResult
        };

    } catch (e: any) {
        console.error('Batch Error:', e);
        return { success: false, error: e.message };
    }
}

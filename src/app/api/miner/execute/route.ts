
import { NextRequest, NextResponse } from 'next/server';
import { runMiningBatch } from '@/utils/batch-runner';
import { getTursoClient, generateUUID, getCurrentTimestamp } from '@/utils/turso';
import { processSeedKeyword, bulkDeferredInsert } from '@/utils/mining-engine';

// Set Vercel Function config
export const maxDuration = 60; // 60 seconds strict
export const dynamic = 'force-dynamic';

/**
 * ğŸ†• íì— ë“±ë¡ëœ ëŒ€ëŸ‰ í‚¤ì›Œë“œ ì²˜ë¦¬ (ì™„ì „ ìˆ˜ì§‘)
 */
async function processQueuedBulkMining(): Promise<any> {
    const db = getTursoClient();
    const startTime = Date.now();
    const MAX_RUN_MS = 55000; // 55ì´ˆ (Vercel 60ì´ˆ ì œí•œ ì „ì— ì¢…ë£Œ)

    // 1. pending ìƒíƒœì˜ í ê°€ì ¸ì˜¤ê¸° (ë¨¼ì € ë“±ë¡ëœ ê²ƒë¶€í„°)
    const queueResult = await db.execute({
        sql: `SELECT * FROM bulk_queue WHERE status = 'pending' ORDER BY created_at ASC LIMIT 1`,
        args: []
    });

    if (queueResult.rows.length === 0) {
        return { message: 'No pending queue items', processed: 0 };
    }

    const queue = queueResult.rows[0];
    const queueId = String(queue.id);
    const seeds = JSON.parse(String(queue.seeds)) as string[];
    const alreadyProcessed = Number(queue.processed_seeds) || 0;

    // ğŸ†• ì´ì–´ì„œ ì²˜ë¦¬: ì´ë¯¸ ì²˜ë¦¬ëœ ì‹œë“œëŠ” ê±´ë„ˆëœ€
    const remainingSeeds = seeds.slice(alreadyProcessed);

    console.log(`[ProcessQueue] Starting queue ${queueId}: ${remainingSeeds.length} remaining (${alreadyProcessed}/${seeds.length} done)`);

    // 2. ìƒíƒœë¥¼ processingìœ¼ë¡œ ì—…ë°ì´íŠ¸
    await db.execute({
        sql: `UPDATE bulk_queue SET status = 'processing', updated_at = ? WHERE id = ?`,
        args: [getCurrentTimestamp(), queueId]
    });

    // 3. ì™„ì „ ìˆ˜ì§‘ íŒŒë¼ë¯¸í„° (ì‹œê°„ ì œí•œ ì—†ìŒ)
    const LIMIT_DOC_COUNT = 0; // ëª¨ë“  í‚¤ì›Œë“œ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
    const MAX_KEYWORDS = 500;
    const MIN_VOLUME = 100;

    let processedSeeds = alreadyProcessed;
    let totalItems = Number(queue.result_count) || 0;
    const allItems: any[] = [];
    let lastError: string | null = null;

    let consecutiveErrors = 0;
    const MAX_CONSECUTIVE_ERRORS = 5; // ì—°ì† 5íšŒ ì—ëŸ¬ ì‹œ í ì‹¤íŒ¨ ì²˜ë¦¬

    for (const seed of remainingSeeds) {
        // ì‹œê°„ ì´ˆê³¼ ì²´í¬
        if (Date.now() - startTime > MAX_RUN_MS) {
            console.log(`[ProcessQueue] Time limit reached, stopping at seed ${processedSeeds}/${seeds.length}`);
            break;
        }

        // ğŸ†• ì—°ì† ì—ëŸ¬ ì œí•œ: ë¬´í•œë£¨í”„ ë°©ì§€
        if (consecutiveErrors >= MAX_CONSECUTIVE_ERRORS) {
            console.error(`[ProcessQueue] Too many consecutive errors (${consecutiveErrors}), marking queue as failed`);
            lastError = `Too many consecutive errors: ${consecutiveErrors}`;
            break;
        }

        try {
            console.log(`[ProcessQueue] Processing seed: ${seed} (${processedSeeds + 1}/${seeds.length})`);

            const result = await processSeedKeyword(
                seed,
                LIMIT_DOC_COUNT,
                false,
                MIN_VOLUME,
                MAX_KEYWORDS
            );

            if (result.items && result.items.length > 0) {
                allItems.push(...result.items);
                totalItems += result.items.length;
            }

            consecutiveErrors = 0; // ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹

        } catch (error: any) {
            console.error(`[ProcessQueue] Error processing seed ${seed}:`, error.message);
            lastError = error.message;
            consecutiveErrors++;
        }

        // ğŸ”´ í•µì‹¬: ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ í•­ìƒ ì‹œë“œ ì¹´ìš´íŠ¸ ì¦ê°€ (ë¬´í•œë£¨í”„ ë°©ì§€)
        processedSeeds++;

        // ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        await db.execute({
            sql: `UPDATE bulk_queue SET processed_seeds = ?, result_count = ?, updated_at = ? WHERE id = ?`,
            args: [processedSeeds, totalItems, getCurrentTimestamp(), queueId]
        });
    }

    // 4. DBì— ì €ì¥ (Deferred Insert)
    if (allItems.length > 0) {
        // ì¤‘ë³µ ì œê±°
        const uniqueMap = new Map<string, any>();
        allItems.forEach(item => {
            const existing = uniqueMap.get(item.keyword);
            if (!existing || (item.total_doc_cnt && !existing.total_doc_cnt)) {
                uniqueMap.set(item.keyword, item);
            }
        });
        const uniqueItems = Array.from(uniqueMap.values());

        await bulkDeferredInsert(uniqueItems);
        console.log(`[ProcessQueue] Saved ${uniqueItems.length} unique keywords to DB`);
    }

    // 5. ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    // ğŸ”´ ìƒíƒœ ê²°ì • ë¡œì§:
    // - ëª¨ë“  ì‹œë“œ ì²˜ë¦¬ ì™„ë£Œ â†’ 'completed'
    // - ì—°ì† ì—ëŸ¬ë¡œ ì¤‘ë‹¨ â†’ 'failed' (ë¬´í•œë£¨í”„ ë°©ì§€)
    // - ì‹œê°„ ì´ˆê³¼ â†’ 'pending' (ë‹¤ìŒ cronì—ì„œ ì´ì–´ì„œ ì²˜ë¦¬)
    let finalStatus: 'completed' | 'pending' | 'failed';
    if (processedSeeds >= seeds.length) {
        finalStatus = 'completed';
    } else if (consecutiveErrors >= MAX_CONSECUTIVE_ERRORS) {
        finalStatus = 'failed';
    } else {
        finalStatus = 'pending';
    }

    await db.execute({
        sql: `UPDATE bulk_queue SET status = ?, processed_seeds = ?, result_count = ?, error = ?, updated_at = ? WHERE id = ?`,
        args: [finalStatus, processedSeeds, totalItems, lastError, getCurrentTimestamp(), queueId]
    });

    const statusMessages = {
        completed: `ì™„ë£Œ: ${seeds.length}ê°œ ì‹œë“œì—ì„œ ${totalItems}ê°œ í‚¤ì›Œë“œ ìˆ˜ì§‘`,
        failed: `ì‹¤íŒ¨: ì—°ì† ${consecutiveErrors}íšŒ ì—ëŸ¬ ë°œìƒ (${processedSeeds}/${seeds.length} ì‹œë“œ ì²˜ë¦¬ë¨)`,
        pending: `ì§„í–‰ ì¤‘: ${processedSeeds}/${seeds.length} ì‹œë“œ ì²˜ë¦¬ (ë‹¤ìŒ cronì—ì„œ ê³„ì†)`
    };

    return {
        queueId,
        status: finalStatus,
        processedSeeds,
        totalSeeds: seeds.length,
        resultCount: totalItems,
        elapsedMs: Date.now() - startTime,
        message: statusMessages[finalStatus]
    };
}

export async function GET(req: NextRequest) {
    // 1. Auth Check
    const authHeader = req.headers.get('Authorization'); // Support Bearer
    const cronHeader = req.headers.get('CRON_SECRET');
    const vercelCronHeader = req.headers.get('x-vercel-cron'); // Vercel Cron ìë™ ì¸ì¦
    const queryKey = req.nextUrl.searchParams.get('key');
    const secret = process.env.CRON_SECRET || 'manual-override-key';

    // Flexible Auth: Vercel Cron (ìë™), Cron Header, Query Param, or Bearer Token
    const isAuthorized = vercelCronHeader === '1' || (cronHeader === secret) || (queryKey === secret) || (authHeader === `Bearer ${secret}`);

    if (!isAuthorized) {
        return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    try {
        // Optional runtime tuning (safe clamps happen inside runMiningBatch)
        const taskParam = (req.nextUrl.searchParams.get('task') || 'all').toLowerCase();
        const task = (taskParam === 'fill_docs' || taskParam === 'expand' || taskParam === 'all' || taskParam === 'process_queue')
            ? (taskParam as 'fill_docs' | 'expand' | 'all' | 'process_queue')
            : 'all';

        // ğŸ†• Process Queue: ë°±ê·¸ë¼ìš´ë“œ ëŒ€ëŸ‰ í‚¤ì›Œë“œ ì™„ì „ ìˆ˜ì§‘
        if (task === 'process_queue') {
            const queueResult = await processQueuedBulkMining();
            return NextResponse.json(queueResult);
        }

        const fillBatch = req.nextUrl.searchParams.get('fillBatch');
        const fillConcurrency = req.nextUrl.searchParams.get('fillConcurrency');
        const seedCount = req.nextUrl.searchParams.get('seedCount');
        const expandBatch = req.nextUrl.searchParams.get('expandBatch');
        const expandConcurrency = req.nextUrl.searchParams.get('expandConcurrency');
        const minSearchVolume = req.nextUrl.searchParams.get('minSearchVolume');
        const maxRunMs = req.nextUrl.searchParams.get('maxRunMs');

        const modeOverrideRaw = (req.nextUrl.searchParams.get('mode') || '').toUpperCase();
        const modeOverride = (modeOverrideRaw === 'NORMAL' || modeOverrideRaw === 'TURBO') ? (modeOverrideRaw as 'NORMAL' | 'TURBO') : undefined;

        // 2. Execute Batch
        const result = await runMiningBatch({
            task,
            mode: modeOverride,
            seedCount: seedCount ? Number(seedCount) : undefined,
            expandBatch: expandBatch ? Number(expandBatch) : undefined,
            expandConcurrency: expandConcurrency ? Number(expandConcurrency) : undefined,
            fillDocsBatch: fillBatch ? Number(fillBatch) : undefined,
            fillDocsConcurrency: fillConcurrency ? Number(fillConcurrency) : undefined,
            minSearchVolume: minSearchVolume ? Number(minSearchVolume) : undefined,
            maxRunMs: maxRunMs ? Number(maxRunMs) : undefined
        });

        // 3. Check for Turbo Mode (Background Recursion)
        const db = getTursoClient();
        const settingResult = await db.execute({
            sql: 'SELECT value FROM settings WHERE key = ?',
            args: ['mining_mode']
        });
        const setting = settingResult.rows.length > 0 ? { value: settingResult.rows[0].value } : null;

        // JSONB ê°’ íŒŒì‹± (getMiningModeì™€ ë™ì¼í•œ ë¡œì§)
        let mode: 'NORMAL' | 'TURBO' = 'TURBO';
        if (setting) {
            const rawValue = (setting as any)?.value;
            if (typeof rawValue === 'string') {
                mode = rawValue.replace(/^"|"$/g, '').toUpperCase() as 'NORMAL' | 'TURBO';
            } else {
                mode = String(rawValue).toUpperCase() as 'NORMAL' | 'TURBO';
            }
            if (mode !== 'NORMAL' && mode !== 'TURBO') {
                mode = 'TURBO';
            }
        }

        console.log(`[Miner] Current mode: ${mode}, Result:`, {
            expand: result.expand?.totalSaved || 0,
            fillDocs: result.fillDocs?.processed || 0
        });

        // NOTE:
        // Vercel serverless functions are not a reliable environment for "fire-and-forget" recursion.
        // We keep the previous recursion behavior behind an explicit env flag so production can safely
        // drive throughput via GitHub Actions loop calls instead.
        const allowSelfSpawn = process.env.TURBO_SELF_SPAWN === '1';

        if (mode === 'TURBO' && allowSelfSpawn) {
            // Check for Stop Conditions (Quota Exhaustion or System Failure)
            const fillErrors = result.fillDocs?.error ? [result.fillDocs.error] : [];
            const expandErrors = result.expand?.details?.filter((d: string) => d.includes('rejected') || d.includes('error')) || [];
            const allErrors = [...fillErrors, ...expandErrors];

            // ê²€ìƒ‰ API í‚¤ ì†Œì§„ ì²´í¬
            const isSearchKeyExhausted = allErrors.some((e: string) =>
                e.includes('No SEARCH keys') ||
                e.includes('All SEARCH keys are rate limited')
            );

            // ê²€ìƒ‰ê´‘ê³  API í‚¤ ì†Œì§„ ì²´í¬
            const isAdKeyExhausted = allErrors.some((e: string) =>
                e.includes('No AD keys') ||
                e.includes('All AD keys are rate limited') ||
                e.includes('Failed to fetch related keywords')
            );

            const totalTried = (result.fillDocs?.processed || 0) + (result.fillDocs?.failed || 0);
            const isTotalFailure = totalTried > 0 && (result.fillDocs?.processed || 0) === 0;

            // API í‚¤ ëª¨ë‘ ì†Œì§„ ë˜ëŠ” ì—°ì† ì‹¤íŒ¨ ì‹œ ìë™ ì¤‘ì§€
            if (isSearchKeyExhausted || isAdKeyExhausted || (isTotalFailure && allErrors.length > 5)) {
                const reason = isSearchKeyExhausted ? 'Search API Keys Exhausted'
                    : isAdKeyExhausted ? 'Ad API Keys Exhausted'
                        : 'High Failure Rate';

                console.warn(`[Miner] TURBO PAUSED: ${reason}. Will retry in next loop.`);

                // âš ï¸ CHANGED: Do NOT disable Turbo Mode. Just stop this specific run.
                // This ensures the loop continues once keys cool down.
                /*
                await db.execute({
                    sql: 'INSERT OR REPLACE INTO settings (key, value, updated_at) VALUES (?, ?, ?)',
                    args: ['mining_mode', 'NORMAL', new Date().toISOString()]
                });
                */

                return NextResponse.json({
                    ...result,
                    info: `Turbo Mode Paused (${reason}). Will retry via cron/loop.`
                });
            }

            const selfUrl = `${req.nextUrl.origin}/api/miner/execute?key=${secret}`;
            console.log(`[Miner] Turbo Mode Active. Spawning next batch: ${selfUrl}`);

            // Spawn next run (best-effort). Keep awaited behavior to avoid unhandled work getting dropped.
            try {
                await fetch(selfUrl, {
                    method: 'GET',
                    headers: { 'CRON_SECRET': secret }
                });
            } catch (err) {
                console.error('[Miner] Failed to spawn next recursion:', err);
            }
        } else {
            // ì¼ë°˜ ëª¨ë“œ: GitHub Actionsê°€ 5ë¶„ë§ˆë‹¤ í˜¸ì¶œí•˜ë¯€ë¡œ ìë™ ìˆ˜ì§‘ ì§„í–‰ ì¤‘
            console.log('[Miner] Normal Mode: Auto-collection via GitHub Actions (every 5 minutes)');
        }

        return NextResponse.json({
            ...result,
            mode: mode,
            info: mode === 'TURBO'
                ? (allowSelfSpawn ? 'Turbo Mode: Continuous background execution' : 'Turbo Mode: Driven by scheduler (GitHub Actions loop recommended)')
                : 'Normal Mode: Scheduled execution via GitHub Actions (every 5 minutes)'
        });
    } catch (e: any) {
        console.error('[Miner] Execution Error:', e);
        return NextResponse.json({ error: e.message }, { status: 500 });
    }
}

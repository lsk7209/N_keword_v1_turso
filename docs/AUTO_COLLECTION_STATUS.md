# 키워드 자동수집 현황 보고서

**생성일**: 2025-01-XX  
**상태**: ✅ 정상 작동 중 (TURBO 모드)

---

## 📊 현재 자동수집 설정

### 1. 실행 스케줄

#### GitHub Actions (`.github/workflows/miner.yml`)
- **실행 주기**: 3분마다 (하루 480회)
- **실행 시간**: 각 실행당 5분간 (300초)
- **호출 간격**: 10초마다 API 호출
- **실행당 호출 횟수**: 약 30회 (300초 ÷ 10초)
- **총 일일 호출**: 약 14,400회 (30회 × 480회)

#### Vercel Cron (`vercel.json`)
- **Expand 작업**: 3개의 cron job (3분마다, 1분씩 오프셋)
  - `*/3 * * * *` (0, 3, 6, 9... 분)
  - `1-59/3 * * * *` (1, 4, 7, 10... 분)
  - `2-59/3 * * * *` (2, 5, 8, 11... 분)
- **Fill Docs 작업**: 2개의 cron job (3분마다, 1분씩 오프셋)
  - `*/3 * * * *` (0, 3, 6, 9... 분)
  - `1-59/3 * * * *` (1, 4, 7, 10... 분)
- **총 일일 실행**: 약 960회 (Expand) + 480회 (Fill Docs) = 1,440회

---

## ⚙️ 수집 파라미터

### Expand 작업 (키워드 확장)
- **모드**: TURBO
- **expandBatch**: 2,000개 시드/실행
- **expandConcurrency**: 130개 동시 처리
- **minSearchVolume**: 1,000 (최소 검색량)
- **maxRunMs**: 58,000ms (58초)
- **skipDocFetch**: true (문서 수집 생략, 속도 최적화)

### Fill Docs 작업 (문서 수집)
- **모드**: TURBO
- **fillBatch**: 5,000개 키워드/실행
- **fillConcurrency**: 480개 동시 처리
- **maxRunMs**: 58,000ms (58초)

---

## 🚀 성능 설정 (TURBO 모드)

### 동적 동시성 계산
- **AD API 키 기반**: `max(20, AD키개수 × 10)`
  - 예: 13개 키 → 130 동시성
- **SEARCH API 키 기반**: `max(50, SEARCH키개수 × 12)`
  - 예: 40개 키 → 480 동시성

### 배치 크기 계산
- **Expand**: `max(200, 동시성 × 20)`
  - 예: 130 동시성 → 2,600 (2,000으로 제한)
- **Fill Docs**: `max(500, 동시성 × 20)`
  - 예: 480 동시성 → 9,600 (5,000으로 제한)

---

## 📈 예상 수집량

### GitHub Actions 기반
- **실행당 시드 처리**: 약 1,500-2,000개 (동시성 고려)
- **실행당 키워드 수집**: 약 30,000-40,000개 (시드당 평균 20개)
- **일일 수집량**: 약 14,400,000-19,200,000개 (이론상)
- **실제 예상**: 약 5,000,000-10,000,000개/일

### Vercel Cron 기반
- **Expand 실행당**: 2,000개 시드 × 20개 키워드 = 40,000개
- **Fill Docs 실행당**: 5,000개 키워드 처리
- **일일 수집량**: 약 38,400,000개 (Expand) + 2,400,000개 (Fill Docs) = 40,800,000개 (이론상)
- **실제 예상**: 약 10,000,000-20,000,000개/일

### 총합
- **일일 수집량 (이론상)**: 약 55,200,000개
- **일일 수집량 (실제 예상)**: 약 15,000,000-30,000,000개
- **시간당 수집량**: 약 625,000-1,250,000개

---

## 🔍 수집 프로세스

### 1. Expand (키워드 확장)
1. DB에서 `is_expanded = 0, 1, 2` 상태의 키워드 중 검색량 1,000 이상인 키워드 선택
2. 최대 2,000개를 `is_expanded = 2` (Processing) 상태로 선점
3. 130개 동시성으로 네이버 연관 검색어 API 호출
4. 수집된 키워드를 DB에 저장 (INSERT OR REPLACE)
5. 성공한 시드는 `is_expanded = 1`로 업데이트

### 2. Fill Docs (문서 수집)
1. DB에서 `total_doc_cnt IS NULL`인 키워드 선택
2. 최대 5,000개를 `is_filling = 1` (Processing) 상태로 선점
3. 480개 동시성으로 네이버 검색 API 호출
4. 문서 수를 DB에 업데이트
5. 성공한 키워드는 `is_filling = 0`으로 업데이트

---

## 🛡️ 안전 장치

### 자동 중지 조건
- **Search API 키 소진**: 모든 SEARCH 키가 rate limit에 도달
- **Ad API 키 소진**: 모든 AD 키가 rate limit에 도달
- **높은 실패율**: 연속 실패 5회 이상
- **자동 조치**: TURBO 모드 → NORMAL 모드로 자동 전환

### 재시도 로직
- **GitHub Actions**: 각 호출당 최대 2회 재시도
- **타임아웃**: 75초 (Vercel 60초 제한 고려)
- **데드라인 스킵**: 58초 경과 시 남은 작업은 다음 실행으로 이월

---

## 📊 모니터링

### 통계 API
- **엔드포인트**: `/api/monitor/stats`
- **제공 정보**:
  - 전체 키워드 수
  - 문서 수 미수집 대기 중
  - 확장 대기 중인 시드 수
  - 최근 24시간 수집량
  - API 키 상태

### 모니터링 페이지
- **경로**: `/monitor`
- **제공 정보**:
  - 전체/분석완료/확장완료 키워드 수
  - 등급별 분포 (PLATINUM, GOLD 등)
  - 최근 24시간 수집량
  - 시드 키워드 현황
  - 최근 수집된 키워드 목록

---

## 🔧 최적화 포인트

### 현재 최적화 상태
- ✅ **Smart Deduplication 비활성화**: DB 읽기 최소화
- ✅ **INSERT OR REPLACE**: 중복 자동 처리
- ✅ **배치 업데이트**: 상태 업데이트를 1회 DB 호출로 처리
- ✅ **동적 동시성**: API 키 개수에 따라 자동 조정
- ✅ **병렬 처리**: 모든 청크를 동시에 처리

### 추가 최적화 가능 영역
1. **API 키 로테이션**: rate limit 회피를 위한 더 정교한 로직
2. **배치 크기 조정**: 실제 처리량에 따라 동적 조정
3. **실패 키워드 재처리**: 실패한 키워드에 대한 지수 백오프 재시도

---

## ⚠️ 주의사항

### Vercel 제한
- **함수 타임아웃**: 60초 (현재 58초로 설정)
- **동시 실행**: 무료 플랜 제한 있음
- **Cron 실행**: 무료 플랜은 제한적 (현재는 Pro 플랜 사용 중으로 보임)

### 네이버 API 제한
- **AD API**: 키당 일일 호출 한도 존재
- **SEARCH API**: 키당 일일 호출 한도 존재
- **Rate Limit**: 초당 호출 제한 존재

### 데이터베이스 부하
- **Turso**: 동시 연결 수 제한
- **트랜잭션**: SQLite 특성상 동시 쓰기 제한적
- **현재 완화**: Smart Deduplication 비활성화로 읽기 부하 감소

---

## 📝 권장 사항

### 단기 (1주일 내)
1. **실제 수집량 모니터링**: `/monitor` 페이지에서 일일 수집량 확인
2. **API 키 상태 확인**: rate limit 도달 여부 모니터링
3. **실패율 확인**: 실패한 키워드 비율 확인 및 원인 분석

### 중기 (1개월 내)
1. **수집량 최적화**: 실제 수집량에 따라 배치 크기/동시성 조정
2. **API 키 추가**: 수집량 증가를 위한 추가 API 키 확보
3. **재시도 로직 개선**: 실패한 키워드에 대한 지능형 재시도

### 장기 (3개월 내)
1. **분산 처리**: 여러 Vercel 프로젝트로 분산하여 수집량 확대
2. **데이터베이스 최적화**: 인덱스 추가, 쿼리 최적화
3. **자동 스케일링**: 수집량에 따라 자동으로 파라미터 조정

---

## 🔗 관련 파일

- **GitHub Actions**: `.github/workflows/miner.yml`
- **Vercel Cron**: `vercel.json`
- **API 엔드포인트**: `src/app/api/miner/execute/route.ts`
- **배치 실행**: `src/utils/batch-runner.ts`
- **수집 엔진**: `src/utils/mining-engine.ts`
- **모니터링**: `src/app/monitor/page.tsx`, `src/app/api/monitor/stats/route.ts`

---

**마지막 업데이트**: 2025-01-XX

